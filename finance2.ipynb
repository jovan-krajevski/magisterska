{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "import yfinance\n",
    "\n",
    "logging.getLogger(\"prophet\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"cmdstanpy\").disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_definition(X, pool_cols, pool_type):\n",
    "    if pool_type == \"complete\":\n",
    "        group = np.zeros(len(X), dtype=\"int\")\n",
    "        group_mapping = {0: \"all\"}\n",
    "        n_groups = 1\n",
    "    else:\n",
    "        X[pool_cols] = pd.Categorical(X[pool_cols])\n",
    "        group = X[pool_cols].cat.codes.values\n",
    "        group_mapping = dict(enumerate(X[pool_cols].cat.categories))\n",
    "        n_groups = X[pool_cols].nunique()\n",
    "    return group, n_groups, group_mapping\n",
    "\n",
    "\n",
    "class TimeSeriesModel:\n",
    "    def _scale_data(self):\n",
    "        self.y_min = 0\n",
    "        self.y_max = self.data[\"y\"].abs().max()\n",
    "        self.ds_min = self.data[\"ds\"].min()\n",
    "        self.ds_max = self.data[\"ds\"].max()\n",
    "\n",
    "        self.data[\"y\"] = self.data[\"y\"] / self.y_max\n",
    "        self.data[\"t\"] = (self.data[\"ds\"] - self.ds_min) / (self.ds_max - self.ds_min)\n",
    "\n",
    "    def _process_data(self):\n",
    "        self.data[\"ds\"] = pd.to_datetime(self.data[\"ds\"])\n",
    "        self.data.sort_values(\"ds\", inplace=True)\n",
    "        self._scale_data()\n",
    "\n",
    "    def _model_init(self):\n",
    "        i0, i1 = self.data[\"ds\"].idxmin(), self.data[\"ds\"].idxmax()\n",
    "        T = self.data[\"t\"].iloc[i1] - self.data[\"t\"].iloc[i0]\n",
    "        slope = (self.data[\"y\"].iloc[i1] - self.data[\"y\"].iloc[i0]) / T\n",
    "        intercept = self.data[\"y\"].iloc[i0] - slope * self.data[\"t\"].iloc[i0]\n",
    "        return {\n",
    "            \"slope\": slope,\n",
    "            \"intercept\": intercept,\n",
    "            \"delta\": 0.0,\n",
    "            \"beta\": 0.0,\n",
    "            \"sigma\": 1.0,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        sigma_sd=0.5,\n",
    "        mcmc_samples=0,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        use_prophet_initvals=True,\n",
    "        progressbar=True,\n",
    "    ):\n",
    "        self.mcmc_samples = mcmc_samples\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self._process_data()\n",
    "\n",
    "        self.initvals = {}\n",
    "        # if use_prophet_initvals:\n",
    "        #     self.initvals = self._model_init()\n",
    "\n",
    "        self.model = pm.Model()\n",
    "        self.model_idxs = {}\n",
    "        mu = self.definition(self.model, self.data, self.initvals, self.model_idxs)\n",
    "\n",
    "        with self.model:\n",
    "            sigma = pm.HalfNormal(\n",
    "                \"sigma\", sigma_sd, initval=self.initvals.get(\"sigma\", 1)\n",
    "            )\n",
    "            _ = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=self.data[\"y\"])\n",
    "\n",
    "            self.map_approx = None\n",
    "            self.trace = None\n",
    "            if self.mcmc_samples == 0:\n",
    "                self.map_approx = pm.find_MAP(progressbar=progressbar, maxeval=1e4)\n",
    "            else:\n",
    "                self.trace = pm.sample(self.mcmc_samples, chains=chains, cores=cores)\n",
    "\n",
    "    def tune(\n",
    "        self,\n",
    "        data,\n",
    "        sigma_sd=0.5,\n",
    "        mcmc_samples=0,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        use_prophet_initvals=True,\n",
    "        progressbar=True,\n",
    "    ):\n",
    "        self.mcmc_samples = mcmc_samples\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self._process_data()\n",
    "\n",
    "        self.initvals = {}\n",
    "        # if use_prophet_initvals:\n",
    "        #     self.initvals = self._model_init()\n",
    "\n",
    "        self.model = pm.Model()\n",
    "        self.model_idxs = {}\n",
    "        # model, data, initvals, model_idxs, prev\n",
    "        mu = self._tune(\n",
    "            self.model, self.data, self.initvals, self.model_idxs, self.map_approx\n",
    "        )\n",
    "\n",
    "        with self.model:\n",
    "            sigma = pm.HalfNormal(\n",
    "                \"sigma\", sigma_sd, initval=self.initvals.get(\"sigma\", 1)\n",
    "            )\n",
    "            _ = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=self.data[\"y\"])\n",
    "\n",
    "            self.map_approx = None\n",
    "            self.trace = None\n",
    "            if self.mcmc_samples == 0:\n",
    "                self.map_approx = pm.find_MAP(progressbar=progressbar, maxeval=1e4)\n",
    "            else:\n",
    "                self.trace = pm.sample(self.mcmc_samples, chains=chains, cores=cores)\n",
    "\n",
    "    def _make_future_df(self, days):\n",
    "        future = pd.DataFrame(\n",
    "            {\n",
    "                \"ds\": pd.DatetimeIndex(\n",
    "                    np.hstack(\n",
    "                        (\n",
    "                            self.data[\"ds\"].unique().to_numpy(),\n",
    "                            pd.date_range(\n",
    "                                self.ds_max,\n",
    "                                self.ds_max + pd.Timedelta(days, \"D\"),\n",
    "                                inclusive=\"right\",\n",
    "                            ).to_numpy(),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        future[\"t\"] = (future[\"ds\"] - self.ds_min) / (self.ds_max - self.ds_min)\n",
    "        return future\n",
    "\n",
    "    def predict(self, days):\n",
    "        future = self._make_future_df(days)\n",
    "        forecasts = self._predict(\n",
    "            future, self.mcmc_samples, self.map_approx, self.trace\n",
    "        )\n",
    "\n",
    "        for group_code in range(forecasts.shape[0]):\n",
    "            future[f\"yhat_{group_code}\"] = forecasts[group_code] * self.y_max\n",
    "            for model_type, model_cnt in self.model_idxs.items():\n",
    "                if model_type.startswith(\"fs\"):\n",
    "                    continue\n",
    "                for model_idx in range(model_cnt):\n",
    "                    component = f\"{model_type}_{model_idx}_{group_code}\"\n",
    "                    if component in future.columns:\n",
    "                        future[component] *= self.y_max\n",
    "\n",
    "        return future\n",
    "\n",
    "    def _predict(self, future, mcmc_samples, map_approx, trace):\n",
    "        if mcmc_samples == 0:\n",
    "            return self._predict_map(future, map_approx)\n",
    "\n",
    "        return self._predict_mcmc(future, trace)\n",
    "\n",
    "    def plot(self, future, y_true=None, pool_cols=None):\n",
    "        plt.figure(figsize=(14, 100 * 6))\n",
    "        plt.subplot(100, 1, 1)\n",
    "        plt.title(\"Predictions\")\n",
    "        plt.grid()\n",
    "\n",
    "        group, _, groups_ = get_group_definition(self.data, pool_cols, \"not_complete\")\n",
    "        for group_code, group_name in groups_.items():\n",
    "            group_idx = group == group_code\n",
    "            color = np.random.rand(3)\n",
    "            plt.scatter(\n",
    "                self.data[\"ds\"][group_idx],\n",
    "                self.data[\"y\"][group_idx] * self.y_max,\n",
    "                s=0.5,\n",
    "                color=color,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        if y_true is not None:\n",
    "            test_group, _, test_groups_ = get_group_definition(\n",
    "                y_true, pool_cols, \"not_complete\"\n",
    "            )\n",
    "            for group_code, group_name in test_groups_.items():\n",
    "                group_idx = test_group == group_code\n",
    "                color = np.random.rand(3)\n",
    "                plt.scatter(\n",
    "                    y_true[\"ds\"][group_idx],\n",
    "                    y_true[\"y\"][group_idx],\n",
    "                    s=0.5,\n",
    "                    color=color,\n",
    "                    label=f\"y - {group_name}\",\n",
    "                )\n",
    "\n",
    "        for group_code, group_name in groups_.items():\n",
    "            plt.plot(\n",
    "                future[\"ds\"],\n",
    "                future[f\"yhat_{group_code}\"],\n",
    "                lw=1,\n",
    "                label=f\"yhat - {group_name}\",\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "        plot_params = {\"idx\": 1}\n",
    "        self._plot(plot_params, future, self.data, self.y_max, y_true)\n",
    "\n",
    "    def metrics(self, y_true, future, pool_cols=None, pool_type=\"individual\"):\n",
    "        metrics = {\"mse\": {}, \"rmse\": {}, \"mae\": {}, \"mape\": {}}\n",
    "        test_group, _, test_groups_ = get_group_definition(y_true, pool_cols, pool_type)\n",
    "        for group_code, group_name in test_groups_.items():\n",
    "            group_idx = test_group == group_code\n",
    "            y = y_true[\"y\"][group_idx]\n",
    "            yhat = future[f\"yhat_{group_code}\"][-len(y) :]\n",
    "            metrics[\"mse\"][group_name] = mean_squared_error(y, yhat)\n",
    "            metrics[\"rmse\"][group_name] = root_mean_squared_error(y, yhat)\n",
    "            metrics[\"mae\"][group_name] = mean_absolute_error(y, yhat)\n",
    "            metrics[\"mape\"][group_name] = mean_absolute_percentage_error(y, yhat)\n",
    "\n",
    "        return pd.DataFrame(metrics)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return AdditiveTimeSeries(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MultiplicativeTimeSeries(self, other)\n",
    "\n",
    "\n",
    "class AdditiveTimeSeries(TimeSeriesModel):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def definition(self, *args, **kwargs):\n",
    "        return self.left.definition(*args, **kwargs) + self.right.definition(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def _tune(self, *args, **kwargs):\n",
    "        return self.left._tune(*args, **kwargs) + self.right._tune(*args, **kwargs)\n",
    "\n",
    "    def _predict(self, *args, **kwargs):\n",
    "        return self.left._predict(*args, **kwargs) + self.right._predict(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def _plot(self, *args, **kwargs):\n",
    "        self.left._plot(*args, **kwargs)\n",
    "        self.right._plot(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.left} + {self.right}\"\n",
    "\n",
    "\n",
    "class MultiplicativeTimeSeries(TimeSeriesModel):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def definition(self, *args, **kwargs):\n",
    "        return self.left.definition(*args, **kwargs) * (\n",
    "            1 + self.right.definition(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _tune(self, *args, **kwargs):\n",
    "        return self.left._tune(*args, **kwargs) * (\n",
    "            1 + self.right._tune(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _predict(self, *args, **kwargs):\n",
    "        return self.left._predict(*args, **kwargs) * (\n",
    "            1 + self.right._predict(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _plot(self, *args, **kwargs):\n",
    "        self.left._plot(*args, **kwargs)\n",
    "        self.right._plot(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        left = f\"{self.left}\"\n",
    "        if type(self.left) is AdditiveTimeSeries:\n",
    "            left = f\"({self.left})\"\n",
    "\n",
    "        right = f\"{self.right}\"\n",
    "        if type(self.right) is AdditiveTimeSeries:\n",
    "            right = f\"({self.right})\"\n",
    "\n",
    "        return f\"{left} * {right}\"\n",
    "\n",
    "\n",
    "class LinearTrend(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_changepoints=25,\n",
    "        changepoint_range=0.8,\n",
    "        slope_mean=0,\n",
    "        slope_sd=5,\n",
    "        intercept_mean=0,\n",
    "        intercept_sd=5,\n",
    "        delta_mean=0,\n",
    "        delta_sd=0.05,\n",
    "        pool_cols=None,\n",
    "        pool_type=\"complete\",\n",
    "        allow_tune=False,\n",
    "    ):\n",
    "        self.n_changepoints = n_changepoints\n",
    "        self.changepoint_range = changepoint_range\n",
    "        self.slope_mean = slope_mean\n",
    "        self.slope_sd = slope_sd\n",
    "        self.intercept_mean = intercept_mean\n",
    "        self.intercept_sd = intercept_sd\n",
    "        self.delta_mean = delta_mean\n",
    "        self.delta_sd = delta_sd\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "        self.allow_tune = allow_tune\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"lt\"] = model_idxs.get(\"lt\", 0)\n",
    "        self.model_idx = model_idxs[\"lt\"]\n",
    "        model_idxs[\"lt\"] += 1\n",
    "\n",
    "        self.group, self.n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                sigma_slope = pm.HalfCauchy(\n",
    "                    f\"lt_{self.model_idx} - sigma_slope\", beta=self.slope_sd\n",
    "                )\n",
    "                offset_slope = pm.Normal(\n",
    "                    f\"lt_{self.model_idx} - offset_slope\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=self.n_groups,\n",
    "                )\n",
    "                slope = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - slope\", offset_slope * sigma_slope\n",
    "                )\n",
    "\n",
    "                delta_sd = self.delta_sd\n",
    "                if self.delta_sd is None:\n",
    "                    delta_sd = pm.Exponential(f\"lt_{self.model_idx} - tau\", 1.5)\n",
    "\n",
    "                sigma_delta = pm.HalfCauchy(\n",
    "                    f\"lt_{self.model_idx} - sigma_delta\", beta=delta_sd\n",
    "                )\n",
    "                offset_delta = pm.Laplace(\n",
    "                    f\"lt_{self.model_idx} - offset_delta\",\n",
    "                    0,\n",
    "                    1,\n",
    "                    shape=(self.n_groups, self.n_changepoints),\n",
    "                )\n",
    "                delta = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - delta\", offset_delta * sigma_delta\n",
    "                )\n",
    "            else:\n",
    "                slope = pm.Normal(\n",
    "                    f\"lt_{self.model_idx} - slope\",\n",
    "                    self.slope_mean,\n",
    "                    self.slope_sd,\n",
    "                    initval=initvals.get(\"slope\", None),\n",
    "                    shape=self.n_groups,\n",
    "                )\n",
    "\n",
    "                delta_sd = self.delta_sd\n",
    "                if self.delta_sd is None:\n",
    "                    delta_sd = pm.Exponential(f\"lt_{self.model_idx} - tau\", 1.5)\n",
    "\n",
    "                delta = pm.Laplace(\n",
    "                    f\"lt_{self.model_idx} - delta\",\n",
    "                    self.delta_mean,\n",
    "                    delta_sd,\n",
    "                    shape=(self.n_groups, self.n_changepoints),\n",
    "                )\n",
    "\n",
    "            intercept = pm.Normal(\n",
    "                f\"lt_{self.model_idx} - intercept\",\n",
    "                self.intercept_mean,\n",
    "                self.intercept_sd,\n",
    "                initval=initvals.get(\"intercept\", None),\n",
    "                shape=self.n_groups,\n",
    "            )\n",
    "\n",
    "            if self.pool_type == \"individual\":\n",
    "                ss = []\n",
    "                t = np.array(data[\"t\"])\n",
    "                for group_code in range(self.n_groups):\n",
    "                    series_data = data[self.group == group_code]\n",
    "                    hist_size = int(\n",
    "                        np.floor(series_data.shape[0] * self.changepoint_range)\n",
    "                    )\n",
    "                    cp_indexes = (\n",
    "                        np.linspace(0, hist_size - 1, self.n_changepoints + 1)\n",
    "                        .round()\n",
    "                        .astype(int)\n",
    "                    )\n",
    "                    ss.append(np.array(series_data.iloc[cp_indexes][\"t\"].tail(-1)))\n",
    "\n",
    "                self.s = np.stack(ss, axis=0)\n",
    "                A = (t[:, None] > self.s[self.group]) * 1\n",
    "\n",
    "                gamma = -self.s[self.group, :] * delta[self.group, :]\n",
    "                trend = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - trend\",\n",
    "                    (slope[self.group] + pm.math.sum(A * delta[self.group], axis=1)) * t\n",
    "                    + (intercept[self.group] + pm.math.sum(A * gamma, axis=1)),\n",
    "                )\n",
    "            else:\n",
    "                t = np.array(data[\"t\"])\n",
    "                hist_size = int(np.floor(data.shape[0] * self.changepoint_range))\n",
    "                cp_indexes = (\n",
    "                    np.linspace(0, hist_size - 1, self.n_changepoints + 1)\n",
    "                    .round()\n",
    "                    .astype(int)\n",
    "                )\n",
    "                self.s = np.array(data.iloc[cp_indexes][\"t\"].tail(-1))\n",
    "                A = (t[:, None] > self.s) * 1\n",
    "\n",
    "                gamma = -self.s * delta[self.group, :]\n",
    "                trend = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - trend\",\n",
    "                    (slope[self.group] + pm.math.sum(A * delta[self.group], axis=1)) * t\n",
    "                    + (intercept[self.group] + pm.math.sum(A * gamma, axis=1)),\n",
    "                )\n",
    "\n",
    "        return trend\n",
    "\n",
    "    def _tune(self, model, data, initvals, model_idxs, prev):\n",
    "        return self.definition(model, data, initvals, model_idxs)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        if self.pool_type != \"individual\":\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s) * 1\n",
    "\n",
    "        for group_code in self.groups_.keys():\n",
    "            if self.pool_type == \"individual\":\n",
    "                s = self.s[group_code]\n",
    "                new_A = (np.array(future[\"t\"])[:, None] > self.s[group_code]) * 1\n",
    "            else:\n",
    "                s = self.s\n",
    "\n",
    "            forecasts.append(\n",
    "                np.array(\n",
    "                    (\n",
    "                        map_approx[f\"lt_{self.model_idx} - slope\"][group_code]\n",
    "                        + np.dot(\n",
    "                            new_A,\n",
    "                            map_approx[f\"lt_{self.model_idx} - delta\"][group_code],\n",
    "                        )\n",
    "                    )\n",
    "                    * future[\"t\"]\n",
    "                    + (\n",
    "                        map_approx[f\"lt_{self.model_idx} - intercept\"][group_code]\n",
    "                        + np.dot(\n",
    "                            new_A,\n",
    "                            (\n",
    "                                -s\n",
    "                                * map_approx[f\"lt_{self.model_idx} - delta\"][group_code]\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            future[f\"lt_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        if self.pool_type == \"individual\":\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s[self.group]) * 1\n",
    "        else:\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s) * 1\n",
    "\n",
    "        for group_code in self.groups_.keys():\n",
    "            delta = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - delta\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "            slope = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - slope\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "            intercept = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - intercept\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "\n",
    "            forecasts.append(\n",
    "                (\n",
    "                    (slope + np.dot(new_A, delta.T)).T * future[\"t\"].to_numpy()\n",
    "                    + (intercept + np.dot(new_A, (-self.s * delta).T)).T\n",
    "                ).mean(0)\n",
    "            )\n",
    "            future[f\"lt_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"lt_{self.model_idx}\")\n",
    "        plt.grid()\n",
    "\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plt.plot(\n",
    "                future[\"ds\"],\n",
    "                future[f\"lt_{self.model_idx}_{group_code}\"],\n",
    "                lw=1,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"LT(n={self.n_changepoints},r={self.changepoint_range},at={self.allow_tune},{self.pool_type})\"\n",
    "        )\n",
    "\n",
    "\n",
    "class FourierSeasonality(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        period,\n",
    "        series_order,\n",
    "        beta_mean=0,\n",
    "        beta_sd=10,\n",
    "        shrinkage_strength=100,\n",
    "        pool_cols=None,\n",
    "        pool_type=\"complete\",\n",
    "        allow_tune=False,\n",
    "    ):\n",
    "        self.period = period\n",
    "        self.series_order = series_order\n",
    "        self.beta_mean = beta_mean\n",
    "        self.beta_sd = beta_sd\n",
    "        self.shrinkage_strength = shrinkage_strength\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "        self.allow_tune = allow_tune\n",
    "\n",
    "    def _fourier_series(self, data):\n",
    "        # convert to days since epoch\n",
    "        NANOSECONDS_TO_SECONDS = 1000 * 1000 * 1000\n",
    "        t = (\n",
    "            data[\"ds\"].to_numpy(dtype=np.int64)\n",
    "            // NANOSECONDS_TO_SECONDS\n",
    "            / (3600 * 24.0)\n",
    "        )\n",
    "\n",
    "        x_T = t * np.pi * 2\n",
    "        fourier_components = np.empty((data[\"ds\"].shape[0], 2 * self.series_order))\n",
    "        for i in range(self.series_order):\n",
    "            c = x_T * (i + 1) / self.period\n",
    "            fourier_components[:, 2 * i] = np.sin(c)\n",
    "            fourier_components[:, (2 * i) + 1] = np.cos(c)\n",
    "\n",
    "        return fourier_components\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"fs\"] = model_idxs.get(\"fs\", 0)\n",
    "        self.model_idx = model_idxs[\"fs\"]\n",
    "        model_idxs[\"fs\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        x = self._fourier_series(data)\n",
    "        beta_initval = initvals.get(\"beta\", None)\n",
    "        if beta_initval is not None:\n",
    "            beta_initval = np.array([beta_initval] * 2 * self.series_order)\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                # shift_t = pm.Uniform(\n",
    "                #     f\"fs_{self.model_idx} - shift_t(p={self.period},n={self.series_order})\",\n",
    "                #     lower=0,\n",
    "                #     upper=self.period,\n",
    "                #     shape=n_groups,\n",
    "                # )\n",
    "                mu_beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - beta_mu(p={self.period},n={self.series_order})\",\n",
    "                    mu=self.beta_mean,\n",
    "                    sigma=self.beta_sd,\n",
    "                    shape=2 * self.series_order,\n",
    "                    initval=beta_initval,\n",
    "                )\n",
    "                sigma_beta = pm.HalfNormal(\n",
    "                    f\"fs_{self.model_idx} - beta_sigma(p={self.period},n={self.series_order})\",\n",
    "                    sigma=self.beta_sd / self.shrinkage_strength,\n",
    "                    shape=2 * self.series_order,\n",
    "                )\n",
    "                offset_beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - offset_beta(p={self.period},n={self.series_order})\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=(n_groups, 2 * self.series_order),\n",
    "                )\n",
    "\n",
    "                beta = pm.Deterministic(\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                    mu_beta + offset_beta * sigma_beta,\n",
    "                )\n",
    "            else:\n",
    "                beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                    mu=self.beta_mean,\n",
    "                    sigma=self.beta_sd,\n",
    "                    shape=(n_groups, 2 * self.series_order),\n",
    "                    initval=beta_initval,\n",
    "                )\n",
    "\n",
    "        return pm.math.sum(x * beta[group], axis=1)\n",
    "\n",
    "    def _tune(self, model, data, initvals, model_idxs, prev):\n",
    "        if not self.allow_tune:\n",
    "            return self.definition(model, data, initvals, model_idxs)\n",
    "\n",
    "        model_idxs[\"fs\"] = model_idxs.get(\"fs\", 0)\n",
    "        self.model_idx = model_idxs[\"fs\"]\n",
    "        model_idxs[\"fs\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        x = self._fourier_series(data)\n",
    "        beta_initval = initvals.get(\"beta\", None)\n",
    "        if beta_initval is not None:\n",
    "            beta_initval = np.array([beta_initval] * 2 * self.series_order)\n",
    "\n",
    "        with model:\n",
    "            sigma_beta = pm.HalfNormal(\n",
    "                f\"fs_{self.model_idx} - beta_sigma(p={self.period},n={self.series_order})\",\n",
    "                sigma=self.beta_sd / self.shrinkage_strength,\n",
    "                shape=2 * self.series_order,\n",
    "            )\n",
    "            offset_beta = pm.Normal(\n",
    "                f\"fs_{self.model_idx} - offset_beta(p={self.period},n={self.series_order})\",\n",
    "                mu=0,\n",
    "                sigma=1,\n",
    "                shape=(n_groups, 2 * self.series_order),\n",
    "            )\n",
    "\n",
    "            beta = pm.Deterministic(\n",
    "                f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                prev[\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                ]\n",
    "                + offset_beta * sigma_beta,\n",
    "            )\n",
    "\n",
    "        return pm.math.sum(x * beta[group], axis=1)\n",
    "\n",
    "    def _det_seasonality_posterior(self, beta, x):\n",
    "        return np.dot(x, beta.T)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                self._det_seasonality_posterior(\n",
    "                    map_approx[\n",
    "                        f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                    ][group_code],\n",
    "                    self._fourier_series(future),\n",
    "                )\n",
    "            )\n",
    "            future[f\"fs_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                self._det_seasonality_posterior(\n",
    "                    trace[\"posterior\"][\n",
    "                        f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                    ]\n",
    "                    .to_numpy()[:, :, group_code]\n",
    "                    .mean(0),\n",
    "                    self._fourier_series(future),\n",
    "                ).T.mean(0)\n",
    "            )\n",
    "            future[f\"fs_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        date = future[\"ds\"] if self.period > 7 else future[\"ds\"].dt.day_name()\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"fs_{self.model_idx} - p={self.period},n={self.series_order}\")\n",
    "        plt.grid()\n",
    "\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plt.plot(\n",
    "                date[-int(self.period) :],\n",
    "                future[f\"fs_{self.model_idx}_{group_code}\"][-int(self.period) :],\n",
    "                lw=1,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"FS(p={self.period},n={self.series_order},at={self.allow_tune},{self.pool_type})\"\n",
    "\n",
    "\n",
    "class Constant(TimeSeriesModel):\n",
    "    def __init__(self, lower, upper, pool_cols=None, pool_type=\"complete\", allow_tune=False):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "        self.allow_tune = allow_tune\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"c\"] = model_idxs.get(\"c\", 0)\n",
    "        self.model_idx = model_idxs[\"c\"]\n",
    "        model_idxs[\"c\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                mu_c = pm.Uniform(\n",
    "                    f\"c_{self.model_idx} - mu_c(l={self.lower},u={self.upper})\",\n",
    "                    lower=self.lower,\n",
    "                    upper=self.upper,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "                offset_c = pm.Normal(\n",
    "                    f\"c_{self.model_idx} - offset_c(l={self.lower},u={self.upper})\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "                c = pm.Deterministic(\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\",\n",
    "                    mu_c + offset_c,\n",
    "                )\n",
    "            else:\n",
    "                c = pm.Uniform(\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\",\n",
    "                    lower=self.lower,\n",
    "                    upper=self.upper,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "\n",
    "        return c[group]\n",
    "    \n",
    "    def _tune(self, model, data, initvals, model_idxs, prev):\n",
    "        return self.definition(model, data, initvals, model_idxs)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                np.ones_like(future[\"t\"])\n",
    "                * map_approx[f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\"][\n",
    "                    group_code\n",
    "                ]\n",
    "            )\n",
    "            future[f\"c_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                np.ones_like(future[\"t\"])\n",
    "                * trace[\"posterior\"][\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\"\n",
    "                ]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean()\n",
    "            )\n",
    "            future[f\"c_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\")\n",
    "\n",
    "        plot_data = []\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plot_data.append(\n",
    "                (group_name, future[f\"c_{self.model_idx}_{group_code}\"][0])\n",
    "            )\n",
    "\n",
    "        plt.bar(*zip(*plot_data))\n",
    "        plt.axhline(0, c=\"k\", linewidth=3)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"C(l={self.lower},u={self.upper},at={self.allow_tune},{self.pool_type})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [\"^W5000\", \"^GSPC\", \"^IXIC\", \"^DJI\"]\n",
    "\n",
    "gspc_tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"AMZN\", \"FB\", \"TSLA\", \"GOOGL\", \"GOOG\", \"JNJ\", \"JPM\", \"V\",\n",
    "    \"PG\", \"UNH\", \"DIS\", \"NVDA\", \"MA\", \"HD\", \"PYPL\", \"VZ\", \"ADBE\", \"CMCSA\",\n",
    "    \"NFLX\", \"BAC\", \"KO\", \"MRK\", \"PEP\", \"T\", \"PFE\", \"INTC\", \"CRM\", \"WMT\", \"ABT\",\n",
    "    \"ABBV\", \"CSCO\", \"TMO\", \"NKE\", \"AVGO\", \"XOM\", \"QCOM\", \"COST\", \"ACN\", \"CVX\",\n",
    "    \"MCD\", \"MDT\", \"NEE\", \"TXN\", \"HON\", \"DHR\", \"UNP\", \"BMY\", \"LIN\", \"LLY\",\n",
    "    \"AMGN\", \"PM\", \"C\", \"SBUX\", \"WFC\", \"ORCL\", \"UPS\", \"LOW\", \"BA\", \"IBM\", \"AMD\",\n",
    "    \"RTX\", \"NOW\", \"BLK\", \"MMM\", \"INTU\", \"AMT\", \"CAT\", \"MS\", \"CHTR\", \"ISRG\",\n",
    "    \"GE\", \"BKNG\", \"GS\", \"CVS\", \"TGT\", \"FIS\", \"LMT\", \"DE\", \"MU\", \"MDLZ\", \"TJX\",\n",
    "    \"SYK\", \"ANTM\", \"SCHW\", \"SPGI\", \"AXP\", \"AMAT\", \"TMUS\", \"ZTS\", \"MO\", \"ADP\",\n",
    "    \"CI\", \"PLD\", \"CL\", \"GILD\", \"BDX\", \"ATVI\", \"CB\", \"CSX\", \"CCI\", \"LRCX\",\n",
    "    \"DUK\", \"ADSK\", \"FISV\", \"CME\", \"SO\", \"ICE\", \"TFC\", \"GPN\", \"USB\", \"EQIX\",\n",
    "    \"PNC\", \"FDX\", \"VRTX\", \"D\", \"APD\", \"NSC\", \"EL\", \"SHW\", \"MMC\", \"ITW\", \"PGR\",\n",
    "    \"EW\", \"ADI\", \"HUM\", \"ILMN\", \"ECL\", \"GM\", \"DD\", \"DG\", \"BSX\", \"REGN\", \"AON\",\n",
    "    \"NEM\", \"EMR\", \"ETN\", \"NOC\", \"MCO\", \"KMB\", \"WM\", \"COF\", \"ROP\", \"CTSH\",\n",
    "    \"ROST\", \"HCA\", \"TWTR\", \"COP\", \"IDXX\", \"EA\", \"AEP\", \"EXC\", \"DOW\", \"BAX\",\n",
    "    \"TEL\", \"KLAC\", \"LHX\", \"SNPS\", \"APH\", \"DLR\", \"CMG\", \"ALGN\", \"CDNS\", \"SYY\",\n",
    "    \"FCX\", \"BIIB\", \"STZ\", \"MSCI\", \"SRE\", \"A\", \"MCHP\", \"GIS\", \"MET\", \"TRV\",\n",
    "    \"DXCM\", \"APTV\", \"PSA\", \"PH\", \"MAR\", \"XEL\", \"TT\", \"CNC\", \"XLNX\", \"GD\", \"BK\",\n",
    "    \"F\", \"IQV\", \"TROW\", \"ALXN\", \"MNST\", \"PPG\", \"HPQ\", \"VRSK\", \"JCI\", \"TDG\",\n",
    "    \"CMI\", \"INFO\", \"ALL\", \"EBAY\", \"ORLY\", \"YUM\", \"AIG\", \"ZBH\", \"SBAC\", \"ANSS\",\n",
    "    \"CTAS\", \"PRU\", \"HLT\", \"RMD\", \"CARR\", \"PSX\", \"BLL\", \"SLB\", \"PCAR\", \"PAYX\",\n",
    "    \"ES\", \"PEG\", \"ROK\", \"EOG\", \"AFL\", \"WEC\", \"CTVA\", \"MSI\", \"WBA\", \"SWK\",\n",
    "    \"ADM\", \"FAST\", \"SPG\", \"MCK\", \"AME\", \"AWK\", \"DFS\", \"LUV\", \"OTIS\", \"GLW\",\n",
    "    \"AZO\", \"VFC\", \"WLTW\", \"MTD\", \"WELL\", \"MPC\", \"KMI\", \"CPRT\", \"STT\", \"DAL\",\n",
    "    \"FRC\", \"CLX\", \"DLTR\", \"SWKS\", \"WY\", \"ED\", \"KR\", \"KEYS\", \"WMB\", \"CERN\",\n",
    "    \"TTWO\", \"FTV\", \"AJG\", \"EIX\", \"MKC\", \"MXIM\", \"LYB\", \"DTE\", \"EFX\", \"VLO\",\n",
    "    \"BBY\", \"AMP\", \"DHI\", \"FLT\", \"VTRS\", \"HSY\", \"KHC\", \"AVB\", \"PAYC\", \"ETSY\",\n",
    "    \"O\", \"VRSN\", \"PPL\", \"CHD\", \"MKTX\", \"ARE\", \"VIAC\", \"CBRE\", \"LEN\", \"WST\",\n",
    "    \"ZBRA\", \"EQR\", \"RSG\", \"SIVB\", \"FTNT\", \"ETR\", \"TER\", \"LH\", \"VMC\", \"FITB\",\n",
    "    \"LVS\", \"IP\", \"NTRS\", \"AEE\", \"TFX\", \"KSU\", \"QRVO\", \"TSN\", \"SYF\", \"CDW\",\n",
    "    \"ODFL\", \"PXD\", \"HOLX\", \"AMCR\", \"GWW\", \"VTR\", \"XYL\", \"DOV\", \"EXPE\", \"GRMN\",\n",
    "    \"COO\", \"CAG\", \"BR\", \"MLM\", \"TYL\", \"HIG\", \"CMS\", \"CTLT\", \"AKAM\", \"OKE\",\n",
    "    \"IR\", \"WDC\", \"URI\", \"HAL\", \"FE\", \"TSCO\", \"MTB\", \"PEAK\", \"INCY\", \"ULTA\",\n",
    "    \"STE\", \"CCL\", \"EXPD\", \"PKI\", \"NUE\", \"DGX\", \"KEY\", \"CTXS\", \"VAR\", \"K\",\n",
    "    \"ANET\", \"CAH\", \"ALB\", \"AES\", \"DRI\", \"KMX\", \"RF\", \"ESS\", \"WAT\", \"CFG\",\n",
    "    \"HPE\", \"NDAQ\", \"CE\", \"DPZ\", \"IEX\", \"EXR\", \"POOL\", \"FMC\", \"DRE\", \"NTAP\",\n",
    "    \"ABMD\", \"OXY\", \"MAA\", \"GPC\", \"TDY\", \"HES\", \"ABC\", \"MAS\", \"IT\", \"NVR\",\n",
    "    \"TIF\", \"J\", \"LDOS\", \"BKR\", \"STX\", \"RCL\", \"EMN\", \"OMC\", \"BXP\", \"SJM\", \"WAB\",\n",
    "    \"HRL\", \"PKG\", \"CINF\", \"AVY\", \"MGM\", \"LNT\", \"HBAN\", \"CHRW\", \"PFG\", \"UAL\",\n",
    "    \"EVRG\", \"BIO\", \"JKHY\", \"NLOK\", \"HAS\", \"ATO\", \"FBHS\", \"CNP\", \"RJF\", \"IFF\",\n",
    "    \"PHM\", \"LW\", \"CXO\", \"XRAY\", \"WRK\", \"JBHT\", \"UDR\", \"WHR\", \"HWM\", \"TXT\",\n",
    "    \"WYNN\", \"FFIV\", \"ALLE\", \"AAP\", \"UHS\", \"L\", \"LYV\", \"HST\", \"CBOE\", \"PWR\",\n",
    "    \"LKQ\", \"FOXA\", \"CPB\", \"AAL\", \"LUMN\", \"HSIC\", \"BWA\", \"RE\", \"WRB\", \"SNA\",\n",
    "    \"IPG\", \"NRG\", \"GL\", \"LNC\", \"WU\", \"PNW\", \"PNR\", \"NI\", \"LB\", \"DVA\", \"ROL\",\n",
    "    \"TPR\", \"TAP\", \"IRM\", \"MHK\", \"CF\", \"AIZ\", \"NCLH\", \"NWL\", \"DISH\", \"IPGP\",\n",
    "    \"MOS\", \"CMA\", \"DISCK\", \"FANG\", \"NLSN\", \"AOS\", \"JNPR\", \"REG\", \"ZION\", \"RHI\",\n",
    "    \"SEE\", \"NWSA\", \"HII\", \"BEN\", \"PVH\", \"IVZ\", \"DXC\", \"COG\", \"KIM\", \"ALK\",\n",
    "    \"PRGO\", \"DVN\", \"LEG\", \"FRT\", \"VNO\", \"FLIR\", \"PBCT\", \"APA\", \"NOV\", \"MRO\",\n",
    "    \"HBI\", \"RL\", \"DISCA\", \"FLS\", \"UNM\", \"VNT\", \"FOX\", \"SLG\", \"GPS\", \"FTI\",\n",
    "    \"XRX\", \"HFC\", \"UAA\", \"UA\", \"NWS\"\n",
    "]\n",
    "\n",
    "dji_tickers = [\n",
    "    \"DIS\", \"WMT\", \"DOW\", \"NKE\", \"CRM\", \"HD\", \"V\", \"MSFT\", \"MMM\", \"CSCO\", \"KO\",\n",
    "    \"AAPL\", \"HON\", \"JNJ\", \"TRV\", \"PG\", \"CVX\", \"VZ\", \"CAT\", \"BA\", \"AMGN\", \"IBM\",\n",
    "    \"AXP\", \"JPM\", \"WBA\", \"MCD\", \"MRK\", \"GS\", \"UNH\", \"INTC\"\n",
    "]\n",
    "\n",
    "ixic_tickers = [\n",
    "    \"FEYE\", \"ATEC\", \"SLAB\", \"CMRX\", \"NVCR\", \"FNLC\", \"NMRK\", \"SCOR\", \"AGLE\",\n",
    "    \"FARO\", \"OLMA\", \"TSLA\", \"FRTA\", \"AKTX\", \"KLXE\", \"CVCO\", \"NVCN\", \"EXAS\",\n",
    "    \"SDC\", \"BBQ\", \"IFRX\", \"CIIC\", \"BBI\", \"FNKO\", \"TWST\", \"FARM\", \"ACCD\",\n",
    "    \"NMRD\", \"FRSX\", \"OPTT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(tickers, start=\"1970-01-01\", end=\"2020-01-01\"):\n",
    "    data = yfinance.download(\n",
    "        tickers,\n",
    "        interval=\"1d\",\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "    downloaded_tickers = {col[1] for col in data.columns}\n",
    "    dfs = []\n",
    "    for ticker in downloaded_tickers:\n",
    "        df = pd.DataFrame(\n",
    "            data={\n",
    "                \"open\": data[\"Open\"][ticker].to_numpy(),\n",
    "                \"high\": data[\"High\"][ticker].to_numpy(),\n",
    "                \"low\": data[\"Low\"][ticker].to_numpy(),\n",
    "                \"close\": data[\"Close\"][ticker].to_numpy(),\n",
    "                \"typical_price\": (\n",
    "                    (\n",
    "                        data[\"Open\"][ticker]\n",
    "                        + data[\"High\"][ticker]\n",
    "                        + data[\"Low\"][ticker]\n",
    "                        + data[\"Close\"][ticker]\n",
    "                    )\n",
    "                    / 4\n",
    "                ).to_numpy(),\n",
    "                \"volume\": data[\"Volume\"][ticker].to_numpy(),\n",
    "            },\n",
    "            index=data[\"Close\"][ticker].index,\n",
    "        )\n",
    "\n",
    "        full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"D\")\n",
    "        df = df.reindex(full_date_range).interpolate()\n",
    "        df[\"ds\"] = df.index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df[\"series\"] = ticker\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_df(\n",
    "    start,\n",
    "    window,\n",
    "    horizon,\n",
    "    dfs,\n",
    "    for_prophet=False,\n",
    "    y_col=\"typical_price\",\n",
    "    perform_scaling=True,\n",
    "):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    scales = []\n",
    "    for df in dfs:\n",
    "        train_df = df[start : start + window].copy()\n",
    "        test_df = df[start + window : start + window + horizon].copy()\n",
    "        if train_df.isna().any().any() or test_df.isna().any().any():\n",
    "            continue\n",
    "\n",
    "        train_df[\"y\"] = train_df[y_col]\n",
    "        test_df[\"y\"] = test_df[y_col]\n",
    "\n",
    "        if perform_scaling:\n",
    "            scales.append(train_df[y_col].max())\n",
    "            train_df[\"y\"] = train_df[y_col] / scales[-1]\n",
    "            test_df[\"y\"] = test_df[y_col] / scales[-1]\n",
    "\n",
    "        train_dfs.append(train_df)\n",
    "        test_dfs.append(test_df)\n",
    "\n",
    "    if len(train_dfs) == 0:\n",
    "        return None\n",
    "\n",
    "    if for_prophet:\n",
    "        return train_dfs, test_dfs, scales\n",
    "\n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs), scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_df_around_point(\n",
    "    window,\n",
    "    horizon,\n",
    "    dfs,\n",
    "    point=\"2009-09-01\",\n",
    "    for_prophet=False,\n",
    "    y_col=\"typical_price\",\n",
    "    perform_scaling=True,\n",
    "):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    scales = []\n",
    "\n",
    "    for df in dfs:\n",
    "        point_idx = len(df[df[\"ds\"] < point])\n",
    "        check = generate_train_test_df(\n",
    "            start=point_idx - window,\n",
    "            window=window,\n",
    "            horizon=horizon,\n",
    "            dfs=[df],\n",
    "            for_prophet=for_prophet,\n",
    "            y_col=y_col,\n",
    "            perform_scaling=perform_scaling,\n",
    "        )\n",
    "        if check is None:\n",
    "            continue\n",
    "\n",
    "        train_df, test_df, scale = check\n",
    "\n",
    "        scales += scale\n",
    "\n",
    "        if for_prophet:\n",
    "            train_dfs += train_df\n",
    "            test_dfs += test_df\n",
    "        else:\n",
    "            train_dfs.append(train_df)\n",
    "            test_dfs.append(test_df)\n",
    "\n",
    "    if len(train_dfs) == 0:\n",
    "        return None\n",
    "    \n",
    "    if for_prophet:\n",
    "        return train_dfs, test_dfs, scales\n",
    "    \n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs), scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_components = [\n",
    "#     [LinearTrend(pool_cols=\"series\", pool_type=pt) for pt in [\"individual\", \"partial\"]],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=365.25, series_order=10, pool_cols=\"series\", pool_type=pt\n",
    "#         )\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=91.3125, series_order=n, pool_cols=\"series\", pool_type=pt\n",
    "#         )\n",
    "#         for n in range(7, 10)\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=30.4375, series_order=n, pool_cols=\"series\", pool_type=pt\n",
    "#         )\n",
    "#         for n in range(4, 7)\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "#     [\n",
    "#         FourierSeasonality(period=7, series_order=3, pool_cols=\"series\", pool_type=pt)\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "model_components = [\n",
    "    [LinearTrend(allow_tune=False)],\n",
    "    [\n",
    "        FourierSeasonality(period=365.25, series_order=10, allow_tune=allow_tune)\n",
    "        for allow_tune in [True, False]\n",
    "    ],\n",
    "    [\n",
    "        FourierSeasonality(period=91.3125, series_order=n, allow_tune=allow_tune)\n",
    "        for n in range(7, 10)\n",
    "        for allow_tune in [True, False]\n",
    "    ],\n",
    "    [\n",
    "        FourierSeasonality(period=30.4375, series_order=n, allow_tune=allow_tune)\n",
    "        for n in range(4, 7)\n",
    "        for allow_tune in [True, False]\n",
    "    ],\n",
    "    [\n",
    "        FourierSeasonality(period=7, series_order=3, allow_tune=allow_tune)\n",
    "        for allow_tune in [True, False]\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [(0, [mc]) for mc in model_components[0]]\n",
    "models = []\n",
    "\n",
    "while len(q):\n",
    "    level, model = q.pop(0)\n",
    "    if level + 1 == len(model_components):\n",
    "        models.append(model)\n",
    "        continue\n",
    "\n",
    "    mcs = model_components[level + 1]\n",
    "    for mc in mcs:\n",
    "        # if mc.pool_type == \"partial\":\n",
    "        #     q.append(\n",
    "        #         (\n",
    "        #             level + 1,\n",
    "        #             model\n",
    "        #             + [\n",
    "        #                 Constant(\n",
    "        #                     lower=-1, upper=1, pool_cols=\"series\", pool_type=\"partial\"\n",
    "        #                 )\n",
    "        #                 * mc\n",
    "        #             ],\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        q.append((level + 1, model + [mc]))\n",
    "        q.append((level + 1, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_models(models):\n",
    "    s = None\n",
    "    for model in models:\n",
    "        if s is None:\n",
    "            s = model\n",
    "        else:\n",
    "            s += model\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model[0] * sum_models(model[1:]) if len(model) > 1 else model[0] for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_models = {\"\"}\n",
    "final_models = []\n",
    "for model in models:\n",
    "    str_model = str(model)\n",
    "    if str_model in str_models:\n",
    "        continue\n",
    "\n",
    "    str_models.add(str_model)\n",
    "    final_models.append(model)\n",
    "\n",
    "len(final_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prophet_metrics(y_trues, y_preds, horizon):\n",
    "    result = None\n",
    "    for y_true, y_pred in zip(y_trues, y_preds):\n",
    "        group_name = y_true[\"series\"].iloc[0]\n",
    "        single_metrics = {\"mse\": {}, \"rmse\": {}, \"mae\": {}, \"mape\": {}}\n",
    "        single_metrics[\"mse\"][group_name] = mean_squared_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        single_metrics[\"rmse\"][group_name] = root_mean_squared_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        single_metrics[\"mae\"][group_name] = mean_absolute_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        single_metrics[\"mape\"][group_name] = mean_absolute_percentage_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        if result is None:\n",
    "            result = pd.DataFrame(single_metrics)\n",
    "        else:\n",
    "            result = pd.concat((result, pd.DataFrame(single_metrics)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  503 of 503 completed\n",
      "\n",
      "83 Failed downloads:\n",
      "['PYPL', 'NCLH', 'HWM', 'CFG', 'AMCR', 'HPE', 'CTLT', 'NWS', 'OTIS', 'APTV', 'UA', 'CARR', 'FANG', 'FOX', 'KEYS', 'HII', 'KHC', 'LB', 'ABBV', 'PAYC', 'LW', 'XYL', 'ZTS', 'ETSY', 'NOW', 'INFO', 'VNT', 'ANET', 'MPC', 'FTV', 'IR', 'CDW', 'ALLE', 'NWSA', 'QRVO', 'HCA', 'DOW', 'FISV', 'KMI', 'CTVA', 'HLT', 'PSX', 'SYF', 'IQV', 'FOXA']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2000-01-01 -> 2011-01-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 946702800, endDate = 1293858000\")')\n",
      "['ANTM', 'FLT', 'PBCT', 'PEAK', 'CXO', 'ABMD', 'DISCK', 'NLOK', 'FRC', 'FB', 'VAR', 'FLIR', 'WLTW', 'ALXN', 'ATVI', 'COG', 'SIVB', 'BLL', 'NLSN', 'TIF', 'TWTR', 'ABC', 'DISCA', 'DRE', 'CERN', 'DISH', 'MXIM', 'CTXS', 'XLNX', 'WRK', 'PKI', 'VIAC', 'KSU', 'RE', 'GPS', 'HFC', 'FBHS', 'PXD']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    }
   ],
   "source": [
    "smp = fetch_data([\"^GSPC\"])\n",
    "smp_tickers = fetch_data(gspc_tickers, start=\"2000-01-01\", end=\"2011-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_smp, test_df_smp, scales_smp = generate_train_test_df_around_point(\n",
    "    window=365 * 35, horizon=365, dfs=smp, for_prophet=True\n",
    ")\n",
    "train_df_ticker, test_df_tickers, scales_tickers = generate_train_test_df_around_point(\n",
    "    window=365 * 1, horizon=365, dfs=smp + smp_tickers, for_prophet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.130289</td>\n",
       "      <td>0.126827</td>\n",
       "      <td>0.181457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse      rmse       mae      mape\n",
       "^GSPC  0.016975  0.130289  0.126827  0.181457"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_prophet = Prophet(seasonality_mode=\"multiplicative\")\n",
    "context_prophet.fit(train_df_smp[0])\n",
    "context_future = context_prophet.make_future_dataframe(\n",
    "    periods=365, include_history=True\n",
    ")\n",
    "context_yhat = context_prophet.predict(context_future)\n",
    "context_metrics = get_prophet_metrics(test_df_smp, [context_yhat], 365)\n",
    "context_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "412it [01:07,  6.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24158762029849162"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_forecasts = []\n",
    "\n",
    "for df, df_test in tqdm(zip(train_df_ticker, test_df_tickers)):\n",
    "    prophet = Prophet(seasonality_mode=\"multiplicative\")\n",
    "    # prophet.add_regressor(\"smp_weekly\", standardize=False, mode=\"additive\")\n",
    "    # prophet.add_regressor(\"smp_yearly\", standardize=False, mode=\"additive\")\n",
    "    # prophet.add_regressor(\"smp_yhat\", standardize=False, mode=\"additive\")\n",
    "\n",
    "    train_df = df.copy()\n",
    "    train_df[\"smp_weekly\"] = context_yhat[\n",
    "        (context_yhat[\"ds\"] >= train_df[\"ds\"].iloc[0])\n",
    "        & (context_yhat[\"ds\"] <= train_df[\"ds\"].iloc[-1])\n",
    "    ][\"weekly\"].to_numpy()\n",
    "    train_df[\"smp_yhat\"] = context_yhat[\n",
    "        (context_yhat[\"ds\"] >= train_df[\"ds\"].iloc[0])\n",
    "        & (context_yhat[\"ds\"] <= train_df[\"ds\"].iloc[-1])\n",
    "    ][\"yhat\"].to_numpy()\n",
    "    train_df[\"smp_yearly\"] = context_yhat[\n",
    "        (context_yhat[\"ds\"] >= train_df[\"ds\"].iloc[0])\n",
    "        & (context_yhat[\"ds\"] <= train_df[\"ds\"].iloc[-1])\n",
    "    ][\"yearly\"].to_numpy()\n",
    "\n",
    "    prophet.fit(train_df)\n",
    "\n",
    "    future = prophet.make_future_dataframe(periods=365, include_history=True)\n",
    "    future[\"smp_weekly\"] = context_yhat[\n",
    "        (context_yhat[\"ds\"] >= future[\"ds\"].iloc[0])\n",
    "        & (context_yhat[\"ds\"] <= future[\"ds\"].iloc[-1])\n",
    "    ][\"weekly\"].to_numpy()\n",
    "    future[\"smp_yhat\"] = context_yhat[\n",
    "        (context_yhat[\"ds\"] >= future[\"ds\"].iloc[0])\n",
    "        & (context_yhat[\"ds\"] <= future[\"ds\"].iloc[-1])\n",
    "    ][\"yhat\"].to_numpy()\n",
    "    future[\"smp_yearly\"] = context_yhat[\n",
    "        (context_yhat[\"ds\"] >= future[\"ds\"].iloc[0])\n",
    "        & (context_yhat[\"ds\"] <= future[\"ds\"].iloc[-1])\n",
    "    ][\"yearly\"].to_numpy()\n",
    "\n",
    "    prophet_forecasts.append(prophet.predict(future))\n",
    "\n",
    "prophet_metrics = get_prophet_metrics(test_df_tickers, prophet_forecasts, 365)\n",
    "prophet_metrics[\"mape\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>0.021052</td>\n",
       "      <td>0.145092</td>\n",
       "      <td>0.108518</td>\n",
       "      <td>0.127334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VTR</th>\n",
       "      <td>0.101716</td>\n",
       "      <td>0.318930</td>\n",
       "      <td>0.273076</td>\n",
       "      <td>0.294286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LYV</th>\n",
       "      <td>0.022714</td>\n",
       "      <td>0.150713</td>\n",
       "      <td>0.116617</td>\n",
       "      <td>0.177962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFX</th>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.095284</td>\n",
       "      <td>0.076339</td>\n",
       "      <td>0.087686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNR</th>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.059005</td>\n",
       "      <td>0.048891</td>\n",
       "      <td>0.058709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>0.069838</td>\n",
       "      <td>0.264270</td>\n",
       "      <td>0.220894</td>\n",
       "      <td>0.210641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBUX</th>\n",
       "      <td>0.241795</td>\n",
       "      <td>0.491727</td>\n",
       "      <td>0.411727</td>\n",
       "      <td>0.330575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIG</th>\n",
       "      <td>0.028214</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.132016</td>\n",
       "      <td>0.363267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETR</th>\n",
       "      <td>0.029872</td>\n",
       "      <td>0.172835</td>\n",
       "      <td>0.148824</td>\n",
       "      <td>0.197391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMAT</th>\n",
       "      <td>0.295822</td>\n",
       "      <td>0.543895</td>\n",
       "      <td>0.477356</td>\n",
       "      <td>0.689059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse      rmse       mae      mape\n",
       "^GSPC  0.021052  0.145092  0.108518  0.127334\n",
       "VTR    0.101716  0.318930  0.273076  0.294286\n",
       "LYV    0.022714  0.150713  0.116617  0.177962\n",
       "TFX    0.009079  0.095284  0.076339  0.087686\n",
       "PNR    0.003482  0.059005  0.048891  0.058709\n",
       "...         ...       ...       ...       ...\n",
       "IBM    0.069838  0.264270  0.220894  0.210641\n",
       "SBUX   0.241795  0.491727  0.411727  0.330575\n",
       "HIG    0.028214  0.167969  0.132016  0.363267\n",
       "ETR    0.029872  0.172835  0.148824  0.197391\n",
       "AMAT   0.295822  0.543895  0.477356  0.689059\n",
       "\n",
       "[412 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(idx, point):\n",
    "    train_df_smp, test_df_smp, scales_smp = generate_train_test_df_around_point(\n",
    "        window=365 * 35, horizon=365, dfs=smp, for_prophet=False, point=point\n",
    "    )\n",
    "    model = final_models[idx]\n",
    "    model.fit(train_df_smp, progressbar=False)\n",
    "    map_approx = model.map_approx\n",
    "    model_metrics = []\n",
    "\n",
    "    for smp_ticker in tqdm(smp + smp_tickers):\n",
    "        check = generate_train_test_df_around_point(\n",
    "            window=365 * 1,\n",
    "            horizon=365,\n",
    "            dfs=[smp_ticker],\n",
    "            for_prophet=False,\n",
    "            point=point,\n",
    "        )\n",
    "        if check is None:\n",
    "            continue\n",
    "\n",
    "        train_df_tickers, test_df_tickers, scales_tickers = check\n",
    "        model.map_approx = map_approx\n",
    "        model.tune(train_df_tickers, progressbar=False)\n",
    "        yhat = model.predict(365)\n",
    "        yhat.to_csv(f\"./out/model_{idx}_series_{smp_ticker['series'].iloc[0]}.csv\")\n",
    "        model_metrics.append(model.metrics(test_df_tickers, yhat, pool_cols=\"series\"))\n",
    "\n",
    "    print(f\"{idx} - {model}: {pd.concat(model_metrics)['mape'].mean()}\")\n",
    "    return pd.concat(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [41:14<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=True,complete) + FS(p=91.3125,n=7,at=True,complete) + FS(p=30.4375,n=4,at=True,complete) + FS(p=7,n=3,at=True,complete)): 0.15715462354219975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 277/504 [26:56<23:56,  6.33s/it] "
     ]
    }
   ],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "point = \"2006-01-01\"\n",
    "all_metrics = []\n",
    "# all_metrics = Parallel(n_jobs=8, prefer=\"threads\")(\n",
    "#     delayed(run_test)(idx, point) for idx, model in enumerate(final_models[:5])\n",
    "# )\n",
    "\n",
    "with open(\"./out/models.txt\", \"w\") as f:\n",
    "    for idx, model in enumerate(final_models):\n",
    "        f.write(f\"model_{idx}: {model}\\n\")\n",
    "\n",
    "for idx, model in enumerate(final_models):\n",
    "    all_metrics.append(run_test(idx, point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.23633406794753192\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=4,complete)): 0.22220762920917422\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=7,n=3,complete)): 0.23315341666383999\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete)): 0.2319121997562418\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.24844633535277494\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=5,complete)): 0.21833104574490267\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.24440633323110628\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=6,complete)): 0.23128528645337754\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.2510987657578827\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=30.4375,n=4,complete)): 0.22092813073161766\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=7,n=3,complete)): 0.32443430023410574\n",
      "LT(n=25,r=0.8,complete) * FS(p=365.25,n=10,complete): 0.26132467662894315\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.2676640206120301\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=30.4375,n=5,complete)): 0.24701435435208458\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.24500685847908377\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=30.4375,n=6,complete)): 0.23043283404499443\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.23279752799390419\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=4,complete)): 0.2635778266024864\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=7,n=3,complete)): 0.22365756636864934\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete)): 0.22104310839182476\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.2525822724714332\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=5,complete)): 0.2891283940374769\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.22541293132577822\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=6,complete)): 0.24130243972704918\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.22127584283102575\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=4,complete)): 0.24580721416671095\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=7,n=3,complete)): 0.2552607177139158\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete)): 0.2517171923887945\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.24750305527454083\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=5,complete)): 0.2072059713698088\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.23187139955817612\n",
      "LT(n=25,r=0.8,complete) * (FS(p=365.25,n=10,complete) + FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=6,complete)): 0.241088262730218\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.202646913584649\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=4,complete)): 0.2025054102499896\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=7,n=3,complete)): 0.2054583165108301\n",
      "LT(n=25,r=0.8,complete) * FS(p=91.3125,n=7,complete): 0.20257263652069823\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.20191592403099326\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=5,complete)): 0.19694630441304153\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.20375659630224463\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=7,complete) + FS(p=30.4375,n=6,complete)): 0.20510383426814857\n",
      "LT(n=25,r=0.8,complete) * (FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.14440515271583015\n",
      "LT(n=25,r=0.8,complete) * FS(p=30.4375,n=4,complete): 0.14596936103923788\n",
      "LT(n=25,r=0.8,complete) * FS(p=7,n=3,complete): 0.15178087662491468\n",
      "LT(n=25,r=0.8,complete): 0.14728593953178326\n",
      "LT(n=25,r=0.8,complete) * (FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.1427020266513023\n",
      "LT(n=25,r=0.8,complete) * FS(p=30.4375,n=5,complete): 0.14468857842530788\n",
      "LT(n=25,r=0.8,complete) * (FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.14461421735978022\n",
      "LT(n=25,r=0.8,complete) * FS(p=30.4375,n=6,complete): 0.14695012766249793\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.20792846473140594\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=4,complete)): 0.20813334747904422\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=7,n=3,complete)): 0.20202021292040367\n",
      "LT(n=25,r=0.8,complete) * FS(p=91.3125,n=8,complete): 0.2028366290338927\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.19713112738236768\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=5,complete)): 0.20039279054041217\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.20368933258959623\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=8,complete) + FS(p=30.4375,n=6,complete)): 0.18722233937734545\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=4,complete) + FS(p=7,n=3,complete)): 0.21114014172149487\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=4,complete)): 0.1974308985701686\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=7,n=3,complete)): 0.19998123277650862\n",
      "LT(n=25,r=0.8,complete) * FS(p=91.3125,n=9,complete): 0.19846155388383105\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=5,complete) + FS(p=7,n=3,complete)): 0.20009961682382296\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=5,complete)): 0.19602751066878155\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=6,complete) + FS(p=7,n=3,complete)): 0.20769376148511803\n",
      "LT(n=25,r=0.8,complete) * (FS(p=91.3125,n=9,complete) + FS(p=30.4375,n=6,complete)): 0.204965389686179\n"
     ]
    }
   ],
   "source": [
    "for metrics, model in zip(all_metrics, final_models):\n",
    "    print(f\"{model}: {metrics[\"mape\"].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.377487</td>\n",
       "      <td>0.614400</td>\n",
       "      <td>0.564832</td>\n",
       "      <td>0.413054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAR</th>\n",
       "      <td>0.111828</td>\n",
       "      <td>0.334407</td>\n",
       "      <td>0.319255</td>\n",
       "      <td>0.312104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIO</th>\n",
       "      <td>0.110366</td>\n",
       "      <td>0.332214</td>\n",
       "      <td>0.308669</td>\n",
       "      <td>0.342185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADSK</th>\n",
       "      <td>0.194641</td>\n",
       "      <td>0.441182</td>\n",
       "      <td>0.413002</td>\n",
       "      <td>0.542215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCO</th>\n",
       "      <td>0.083241</td>\n",
       "      <td>0.288515</td>\n",
       "      <td>0.263073</td>\n",
       "      <td>0.423926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEE</th>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.082679</td>\n",
       "      <td>0.069304</td>\n",
       "      <td>0.078973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.210672</td>\n",
       "      <td>0.192508</td>\n",
       "      <td>0.177721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VMC</th>\n",
       "      <td>0.060941</td>\n",
       "      <td>0.246863</td>\n",
       "      <td>0.230722</td>\n",
       "      <td>0.369754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mse      rmse       mae      mape\n",
       "AMZN  0.377487  0.614400  0.564832  0.413054\n",
       "MAR   0.111828  0.334407  0.319255  0.312104\n",
       "BIO   0.110366  0.332214  0.308669  0.342185\n",
       "ADSK  0.194641  0.441182  0.413002  0.542215\n",
       "MCO   0.083241  0.288515  0.263073  0.423926\n",
       "NEE   0.006836  0.082679  0.069304  0.078973\n",
       "MO    0.044383  0.210672  0.192508  0.177721\n",
       "VMC   0.060941  0.246863  0.230722  0.369754"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse     0.051284\n",
       "rmse    0.204085\n",
       "mae     0.169703\n",
       "mape    0.204375\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_metrics.iloc[:9].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
