{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "import yfinance\n",
    "\n",
    "logging.getLogger(\"prophet\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"cmdstanpy\").disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_definition(X, pool_cols, pool_type):\n",
    "    if pool_type == \"complete\":\n",
    "        group = np.zeros(len(X), dtype=\"int\")\n",
    "        group_mapping = {0: \"all\"}\n",
    "        n_groups = 1\n",
    "    else:\n",
    "        X[pool_cols] = pd.Categorical(X[pool_cols])\n",
    "        group = X[pool_cols].cat.codes.values\n",
    "        group_mapping = dict(enumerate(X[pool_cols].cat.categories))\n",
    "        n_groups = X[pool_cols].nunique()\n",
    "    return group, n_groups, group_mapping\n",
    "\n",
    "\n",
    "class TimeSeriesModel:\n",
    "    def _scale_data(self):\n",
    "        self.y_min = 0\n",
    "        self.y_max = self.data[\"y\"].abs().max()\n",
    "        self.ds_min = self.data[\"ds\"].min()\n",
    "        self.ds_max = self.data[\"ds\"].max()\n",
    "\n",
    "        self.data[\"y\"] = self.data[\"y\"] / self.y_max\n",
    "        self.data[\"t\"] = (self.data[\"ds\"] - self.ds_min) / (self.ds_max - self.ds_min)\n",
    "\n",
    "    def _process_data(self):\n",
    "        self.data[\"ds\"] = pd.to_datetime(self.data[\"ds\"])\n",
    "        self.data.sort_values(\"ds\", inplace=True)\n",
    "        self._scale_data()\n",
    "\n",
    "    def _model_init(self):\n",
    "        i0, i1 = self.data[\"ds\"].idxmin(), self.data[\"ds\"].idxmax()\n",
    "        T = self.data[\"t\"].iloc[i1] - self.data[\"t\"].iloc[i0]\n",
    "        slope = (self.data[\"y\"].iloc[i1] - self.data[\"y\"].iloc[i0]) / T\n",
    "        intercept = self.data[\"y\"].iloc[i0] - slope * self.data[\"t\"].iloc[i0]\n",
    "        return {\n",
    "            \"slope\": slope,\n",
    "            \"intercept\": intercept,\n",
    "            \"delta\": 0.0,\n",
    "            \"beta\": 0.0,\n",
    "            \"sigma\": 1.0,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        sigma_sd=0.5,\n",
    "        mcmc_samples=0,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        use_prophet_initvals=True,\n",
    "        progressbar=True,\n",
    "    ):\n",
    "        self.mcmc_samples = mcmc_samples\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self._process_data()\n",
    "\n",
    "        self.initvals = {}\n",
    "        if use_prophet_initvals:\n",
    "            self.initvals = self._model_init()\n",
    "\n",
    "        self.model = pm.Model()\n",
    "        self.model_idxs = {}\n",
    "        mu = self.definition(self.model, self.data, self.initvals, self.model_idxs)\n",
    "\n",
    "        with self.model:\n",
    "            sigma = pm.HalfNormal(\n",
    "                \"sigma\", sigma_sd, initval=self.initvals.get(\"sigma\", 1)\n",
    "            )\n",
    "            _ = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=self.data[\"y\"])\n",
    "\n",
    "            self.map_approx = None\n",
    "            self.trace = None\n",
    "            if self.mcmc_samples == 0:\n",
    "                self.map_approx = pm.find_MAP(progressbar=progressbar, maxeval=1e4)\n",
    "            else:\n",
    "                self.trace = pm.sample(self.mcmc_samples, chains=chains, cores=cores)\n",
    "\n",
    "    def tune(\n",
    "        self,\n",
    "        data,\n",
    "        sigma_sd=0.5,\n",
    "        mcmc_samples=0,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        use_prophet_initvals=True,\n",
    "        progressbar=True,\n",
    "    ):\n",
    "        self.mcmc_samples = mcmc_samples\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self._process_data()\n",
    "\n",
    "        self.initvals = {}\n",
    "        if use_prophet_initvals:\n",
    "            self.initvals = self._model_init()\n",
    "\n",
    "        self.model = pm.Model()\n",
    "        self.model_idxs = {}\n",
    "        mu = self._tune(\n",
    "            self.model, self.data, self.initvals, self.model_idxs, self.map_approx\n",
    "        )\n",
    "\n",
    "        with self.model:\n",
    "            sigma = pm.HalfNormal(\n",
    "                \"sigma\", sigma_sd, initval=self.initvals.get(\"sigma\", 1)\n",
    "            )\n",
    "            _ = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=self.data[\"y\"])\n",
    "\n",
    "            self.map_approx = None\n",
    "            self.trace = None\n",
    "            if self.mcmc_samples == 0:\n",
    "                self.map_approx = pm.find_MAP(progressbar=progressbar, maxeval=1e4)\n",
    "            else:\n",
    "                self.trace = pm.sample(self.mcmc_samples, chains=chains, cores=cores)\n",
    "\n",
    "    def _make_future_df(self, days):\n",
    "        future = pd.DataFrame(\n",
    "            {\n",
    "                \"ds\": pd.DatetimeIndex(\n",
    "                    np.hstack(\n",
    "                        (\n",
    "                            self.data[\"ds\"].unique().to_numpy(),\n",
    "                            pd.date_range(\n",
    "                                self.ds_max,\n",
    "                                self.ds_max + pd.Timedelta(days, \"D\"),\n",
    "                                inclusive=\"right\",\n",
    "                            ).to_numpy(),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        future[\"t\"] = (future[\"ds\"] - self.ds_min) / (self.ds_max - self.ds_min)\n",
    "        return future\n",
    "\n",
    "    def predict(self, days):\n",
    "        future = self._make_future_df(days)\n",
    "        forecasts = self._predict(\n",
    "            future, self.mcmc_samples, self.map_approx, self.trace\n",
    "        )\n",
    "\n",
    "        for group_code in range(forecasts.shape[0]):\n",
    "            future[f\"yhat_{group_code}\"] = forecasts[group_code] * self.y_max\n",
    "            for model_type, model_cnt in self.model_idxs.items():\n",
    "                if model_type.startswith(\"fs\"):\n",
    "                    continue\n",
    "                for model_idx in range(model_cnt):\n",
    "                    component = f\"{model_type}_{model_idx}_{group_code}\"\n",
    "                    if component in future.columns:\n",
    "                        future[component] *= self.y_max\n",
    "\n",
    "        return future\n",
    "\n",
    "    def _predict(self, future, mcmc_samples, map_approx, trace):\n",
    "        if mcmc_samples == 0:\n",
    "            return self._predict_map(future, map_approx)\n",
    "\n",
    "        return self._predict_mcmc(future, trace)\n",
    "\n",
    "    def plot(self, future, y_true=None, pool_cols=None):\n",
    "        plt.figure(figsize=(14, 100 * 6))\n",
    "        plt.subplot(100, 1, 1)\n",
    "        plt.title(\"Predictions\")\n",
    "        plt.grid()\n",
    "\n",
    "        group, _, groups_ = get_group_definition(self.data, pool_cols, \"not_complete\")\n",
    "        for group_code, group_name in groups_.items():\n",
    "            group_idx = group == group_code\n",
    "            color = np.random.rand(3)\n",
    "            plt.scatter(\n",
    "                self.data[\"ds\"][group_idx],\n",
    "                self.data[\"y\"][group_idx] * self.y_max,\n",
    "                s=0.5,\n",
    "                color=color,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        if y_true is not None:\n",
    "            test_group, _, test_groups_ = get_group_definition(\n",
    "                y_true, pool_cols, \"not_complete\"\n",
    "            )\n",
    "            for group_code, group_name in test_groups_.items():\n",
    "                group_idx = test_group == group_code\n",
    "                color = np.random.rand(3)\n",
    "                plt.scatter(\n",
    "                    y_true[\"ds\"][group_idx],\n",
    "                    y_true[\"y\"][group_idx],\n",
    "                    s=0.5,\n",
    "                    color=color,\n",
    "                    label=f\"y - {group_name}\",\n",
    "                )\n",
    "\n",
    "        for group_code, group_name in groups_.items():\n",
    "            plt.plot(\n",
    "                future[\"ds\"],\n",
    "                future[f\"yhat_{group_code}\"],\n",
    "                lw=1,\n",
    "                label=f\"yhat - {group_name}\",\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "        plot_params = {\"idx\": 1}\n",
    "        self._plot(plot_params, future, self.data, self.y_max, y_true)\n",
    "\n",
    "    def metrics(self, y_true, future, pool_cols=None, pool_type=\"individual\"):\n",
    "        metrics = {\"mse\": {}, \"rmse\": {}, \"mae\": {}, \"mape\": {}}\n",
    "        test_group, _, test_groups_ = get_group_definition(y_true, pool_cols, pool_type)\n",
    "        for group_code, group_name in test_groups_.items():\n",
    "            group_idx = test_group == group_code\n",
    "            y = y_true[\"y\"][group_idx]\n",
    "            yhat = future[f\"yhat_{group_code}\"][-len(y) :]\n",
    "            metrics[\"mse\"][group_name] = mean_squared_error(y, yhat)\n",
    "            metrics[\"rmse\"][group_name] = root_mean_squared_error(y, yhat)\n",
    "            metrics[\"mae\"][group_name] = mean_absolute_error(y, yhat)\n",
    "            metrics[\"mape\"][group_name] = mean_absolute_percentage_error(y, yhat)\n",
    "\n",
    "        return pd.DataFrame(metrics)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return AdditiveTimeSeries(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MultiplicativeTimeSeries(self, other)\n",
    "\n",
    "\n",
    "class AdditiveTimeSeries(TimeSeriesModel):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def definition(self, *args, **kwargs):\n",
    "        return self.left.definition(*args, **kwargs) + self.right.definition(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def _tune(self, *args, **kwargs):\n",
    "        return self.left._tune(*args, **kwargs) + self.right._tune(*args, **kwargs)\n",
    "\n",
    "    def _predict(self, *args, **kwargs):\n",
    "        return self.left._predict(*args, **kwargs) + self.right._predict(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def _plot(self, *args, **kwargs):\n",
    "        self.left._plot(*args, **kwargs)\n",
    "        self.right._plot(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.left} + {self.right}\"\n",
    "\n",
    "\n",
    "class MultiplicativeTimeSeries(TimeSeriesModel):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def definition(self, *args, **kwargs):\n",
    "        return self.left.definition(*args, **kwargs) * (\n",
    "            1 + self.right.definition(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _tune(self, *args, **kwargs):\n",
    "        return self.left._tune(*args, **kwargs) * (\n",
    "            1 + self.right._tune(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _predict(self, *args, **kwargs):\n",
    "        return self.left._predict(*args, **kwargs) * (\n",
    "            1 + self.right._predict(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _plot(self, *args, **kwargs):\n",
    "        self.left._plot(*args, **kwargs)\n",
    "        self.right._plot(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        left = f\"{self.left}\"\n",
    "        if type(self.left) is AdditiveTimeSeries:\n",
    "            left = f\"({self.left})\"\n",
    "\n",
    "        right = f\"{self.right}\"\n",
    "        if type(self.right) is AdditiveTimeSeries:\n",
    "            right = f\"({self.right})\"\n",
    "\n",
    "        return f\"{left} * {right}\"\n",
    "\n",
    "\n",
    "class LinearTrend(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_changepoints=25,\n",
    "        changepoint_range=0.8,\n",
    "        slope_mean=0,\n",
    "        slope_sd=5,\n",
    "        intercept_mean=0,\n",
    "        intercept_sd=5,\n",
    "        delta_mean=0,\n",
    "        delta_sd=0.05,\n",
    "        pool_cols=None,\n",
    "        pool_type=\"complete\",\n",
    "        allow_tune=False,\n",
    "    ):\n",
    "        self.n_changepoints = n_changepoints\n",
    "        self.changepoint_range = changepoint_range\n",
    "        self.slope_mean = slope_mean\n",
    "        self.slope_sd = slope_sd\n",
    "        self.intercept_mean = intercept_mean\n",
    "        self.intercept_sd = intercept_sd\n",
    "        self.delta_mean = delta_mean\n",
    "        self.delta_sd = delta_sd\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "        self.allow_tune = allow_tune\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"lt\"] = model_idxs.get(\"lt\", 0)\n",
    "        self.model_idx = model_idxs[\"lt\"]\n",
    "        model_idxs[\"lt\"] += 1\n",
    "\n",
    "        self.group, self.n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        slope_initval = initvals.get(\"slope\", None)\n",
    "        if slope_initval is not None:\n",
    "            slope_initval = np.array([slope_initval] * self.n_groups)\n",
    "\n",
    "        delta_initval = initvals.get(\"delta\", None)\n",
    "        if delta_initval is not None:\n",
    "            delta_initval = np.array(\n",
    "                [[delta_initval] * self.n_changepoints] * self.n_groups\n",
    "            )\n",
    "\n",
    "        intercept_initval = initvals.get(\"intercept\", None)\n",
    "        if intercept_initval is not None:\n",
    "            intercept_initval = np.array([intercept_initval] * self.n_groups)\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                sigma_slope = pm.HalfCauchy(\n",
    "                    f\"lt_{self.model_idx} - sigma_slope\", beta=self.slope_sd\n",
    "                )\n",
    "                offset_slope = pm.Normal(\n",
    "                    f\"lt_{self.model_idx} - offset_slope\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=self.n_groups,\n",
    "                )\n",
    "                slope = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - slope\", offset_slope * sigma_slope\n",
    "                )\n",
    "\n",
    "                delta_sd = self.delta_sd\n",
    "                if self.delta_sd is None:\n",
    "                    delta_sd = pm.Exponential(f\"lt_{self.model_idx} - tau\", 1.5)\n",
    "\n",
    "                sigma_delta = pm.HalfCauchy(\n",
    "                    f\"lt_{self.model_idx} - sigma_delta\", beta=delta_sd\n",
    "                )\n",
    "                offset_delta = pm.Laplace(\n",
    "                    f\"lt_{self.model_idx} - offset_delta\",\n",
    "                    0,\n",
    "                    1,\n",
    "                    shape=(self.n_groups, self.n_changepoints),\n",
    "                )\n",
    "                delta = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - delta\", offset_delta * sigma_delta\n",
    "                )\n",
    "            else:\n",
    "                slope = pm.Normal(\n",
    "                    f\"lt_{self.model_idx} - slope\",\n",
    "                    self.slope_mean,\n",
    "                    self.slope_sd,\n",
    "                    initval=slope_initval,\n",
    "                    shape=self.n_groups,\n",
    "                )\n",
    "\n",
    "                delta_sd = self.delta_sd\n",
    "                if self.delta_sd is None:\n",
    "                    delta_sd = pm.Exponential(f\"lt_{self.model_idx} - tau\", 1.5)\n",
    "\n",
    "                delta = pm.Laplace(\n",
    "                    f\"lt_{self.model_idx} - delta\",\n",
    "                    self.delta_mean,\n",
    "                    delta_sd,\n",
    "                    initval=delta_initval,\n",
    "                    shape=(self.n_groups, self.n_changepoints),\n",
    "                )\n",
    "\n",
    "            intercept = pm.Normal(\n",
    "                f\"lt_{self.model_idx} - intercept\",\n",
    "                self.intercept_mean,\n",
    "                self.intercept_sd,\n",
    "                initval=intercept_initval,\n",
    "                shape=self.n_groups,\n",
    "            )\n",
    "\n",
    "            if self.pool_type == \"individual\":\n",
    "                ss = []\n",
    "                t = np.array(data[\"t\"])\n",
    "                for group_code in range(self.n_groups):\n",
    "                    series_data = data[self.group == group_code]\n",
    "                    hist_size = int(\n",
    "                        np.floor(series_data.shape[0] * self.changepoint_range)\n",
    "                    )\n",
    "                    cp_indexes = (\n",
    "                        np.linspace(0, hist_size - 1, self.n_changepoints + 1)\n",
    "                        .round()\n",
    "                        .astype(int)\n",
    "                    )\n",
    "                    ss.append(np.array(series_data.iloc[cp_indexes][\"t\"].tail(-1)))\n",
    "\n",
    "                self.s = np.stack(ss, axis=0)\n",
    "                A = (t[:, None] > self.s[self.group]) * 1\n",
    "\n",
    "                gamma = -self.s[self.group, :] * delta[self.group, :]\n",
    "                trend = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - trend\",\n",
    "                    (slope[self.group] + pm.math.sum(A * delta[self.group], axis=1)) * t\n",
    "                    + (intercept[self.group] + pm.math.sum(A * gamma, axis=1)),\n",
    "                )\n",
    "            else:\n",
    "                t = np.array(data[\"t\"])\n",
    "                hist_size = int(np.floor(data.shape[0] * self.changepoint_range))\n",
    "                cp_indexes = (\n",
    "                    np.linspace(0, hist_size - 1, self.n_changepoints + 1)\n",
    "                    .round()\n",
    "                    .astype(int)\n",
    "                )\n",
    "                self.s = np.array(data.iloc[cp_indexes][\"t\"].tail(-1))\n",
    "                A = (t[:, None] > self.s) * 1\n",
    "\n",
    "                gamma = -self.s * delta[self.group, :]\n",
    "                trend = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - trend\",\n",
    "                    (slope[self.group] + pm.math.sum(A * delta[self.group], axis=1)) * t\n",
    "                    + (intercept[self.group] + pm.math.sum(A * gamma, axis=1)),\n",
    "                )\n",
    "\n",
    "        return trend\n",
    "\n",
    "    def _tune(self, model, data, initvals, model_idxs, prev):\n",
    "        return self.definition(model, data, initvals, model_idxs)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        if self.pool_type != \"individual\":\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s) * 1\n",
    "\n",
    "        for group_code in self.groups_.keys():\n",
    "            if self.pool_type == \"individual\":\n",
    "                s = self.s[group_code]\n",
    "                new_A = (np.array(future[\"t\"])[:, None] > self.s[group_code]) * 1\n",
    "            else:\n",
    "                s = self.s\n",
    "\n",
    "            forecasts.append(\n",
    "                np.array(\n",
    "                    (\n",
    "                        map_approx[f\"lt_{self.model_idx} - slope\"][group_code]\n",
    "                        + np.dot(\n",
    "                            new_A,\n",
    "                            map_approx[f\"lt_{self.model_idx} - delta\"][group_code],\n",
    "                        )\n",
    "                    )\n",
    "                    * future[\"t\"]\n",
    "                    + (\n",
    "                        map_approx[f\"lt_{self.model_idx} - intercept\"][group_code]\n",
    "                        + np.dot(\n",
    "                            new_A,\n",
    "                            (\n",
    "                                -s\n",
    "                                * map_approx[f\"lt_{self.model_idx} - delta\"][group_code]\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            future[f\"lt_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        if self.pool_type == \"individual\":\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s[self.group]) * 1\n",
    "        else:\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s) * 1\n",
    "\n",
    "        for group_code in self.groups_.keys():\n",
    "            delta = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - delta\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "            slope = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - slope\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "            intercept = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - intercept\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "\n",
    "            forecasts.append(\n",
    "                (\n",
    "                    (slope + np.dot(new_A, delta.T)).T * future[\"t\"].to_numpy()\n",
    "                    + (intercept + np.dot(new_A, (-self.s * delta).T)).T\n",
    "                ).mean(0)\n",
    "            )\n",
    "            future[f\"lt_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"lt_{self.model_idx}\")\n",
    "        plt.grid()\n",
    "\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plt.plot(\n",
    "                future[\"ds\"],\n",
    "                future[f\"lt_{self.model_idx}_{group_code}\"],\n",
    "                lw=1,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LT(n={self.n_changepoints},r={self.changepoint_range},at={self.allow_tune},{self.pool_type})\"\n",
    "\n",
    "\n",
    "class FourierSeasonality(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        period,\n",
    "        series_order,\n",
    "        beta_mean=0,\n",
    "        beta_sd=10,\n",
    "        shrinkage_strength=100,\n",
    "        pool_cols=None,\n",
    "        pool_type=\"complete\",\n",
    "        allow_tune=False,\n",
    "    ):\n",
    "        self.period = period\n",
    "        self.series_order = series_order\n",
    "        self.beta_mean = beta_mean\n",
    "        self.beta_sd = beta_sd\n",
    "        self.shrinkage_strength = shrinkage_strength\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "        self.allow_tune = allow_tune\n",
    "\n",
    "    def _fourier_series(self, data):\n",
    "        # convert to days since epoch\n",
    "        NANOSECONDS_TO_SECONDS = 1000 * 1000 * 1000\n",
    "        t = (\n",
    "            data[\"ds\"].to_numpy(dtype=np.int64)\n",
    "            // NANOSECONDS_TO_SECONDS\n",
    "            / (3600 * 24.0)\n",
    "        )\n",
    "\n",
    "        x_T = t * np.pi * 2\n",
    "        fourier_components = np.empty((data[\"ds\"].shape[0], 2 * self.series_order))\n",
    "        for i in range(self.series_order):\n",
    "            c = x_T * (i + 1) / self.period\n",
    "            fourier_components[:, 2 * i] = np.sin(c)\n",
    "            fourier_components[:, (2 * i) + 1] = np.cos(c)\n",
    "\n",
    "        return fourier_components\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"fs\"] = model_idxs.get(\"fs\", 0)\n",
    "        self.model_idx = model_idxs[\"fs\"]\n",
    "        model_idxs[\"fs\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        x = self._fourier_series(data)\n",
    "        beta_initval = initvals.get(\"beta\", None)\n",
    "        if beta_initval is not None:\n",
    "            if self.pool_type == \"partial\":\n",
    "                beta_initval = np.array([beta_initval] * 2 * self.series_order)\n",
    "            else:\n",
    "                beta_initval = np.array(\n",
    "                    [[beta_initval] * 2 * self.series_order] * n_groups\n",
    "                )\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                # shift_t = pm.Uniform(\n",
    "                #     f\"fs_{self.model_idx} - shift_t(p={self.period},n={self.series_order})\",\n",
    "                #     lower=0,\n",
    "                #     upper=self.period,\n",
    "                #     shape=n_groups,\n",
    "                # )\n",
    "                mu_beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - beta_mu(p={self.period},n={self.series_order})\",\n",
    "                    mu=self.beta_mean,\n",
    "                    sigma=self.beta_sd,\n",
    "                    shape=2 * self.series_order,\n",
    "                    initval=beta_initval,\n",
    "                )\n",
    "                sigma_beta = pm.HalfNormal(\n",
    "                    f\"fs_{self.model_idx} - beta_sigma(p={self.period},n={self.series_order})\",\n",
    "                    sigma=self.beta_sd / self.shrinkage_strength,\n",
    "                    shape=2 * self.series_order,\n",
    "                )\n",
    "                offset_beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - offset_beta(p={self.period},n={self.series_order})\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=(n_groups, 2 * self.series_order),\n",
    "                )\n",
    "\n",
    "                beta = pm.Deterministic(\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                    mu_beta + offset_beta * sigma_beta,\n",
    "                )\n",
    "            else:\n",
    "                beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                    mu=self.beta_mean,\n",
    "                    sigma=self.beta_sd,\n",
    "                    shape=(n_groups, 2 * self.series_order),\n",
    "                    initval=beta_initval,\n",
    "                )\n",
    "\n",
    "        return pm.math.sum(x * beta[group], axis=1)\n",
    "\n",
    "    def _tune(self, model, data, initvals, model_idxs, prev):\n",
    "        if not self.allow_tune:\n",
    "            return self.definition(model, data, initvals, model_idxs)\n",
    "\n",
    "        model_idxs[\"fs\"] = model_idxs.get(\"fs\", 0)\n",
    "        self.model_idx = model_idxs[\"fs\"]\n",
    "        model_idxs[\"fs\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        x = self._fourier_series(data)\n",
    "        beta_initval = initvals.get(\"beta\", None)\n",
    "        if beta_initval is not None:\n",
    "            beta_initval = np.array([beta_initval] * 2 * self.series_order)\n",
    "\n",
    "        with model:\n",
    "            sigma_beta = pm.HalfNormal(\n",
    "                f\"fs_{self.model_idx} - beta_sigma(p={self.period},n={self.series_order})\",\n",
    "                sigma=self.beta_sd / self.shrinkage_strength,\n",
    "                shape=2 * self.series_order,\n",
    "            )\n",
    "            offset_beta = pm.Normal(\n",
    "                f\"fs_{self.model_idx} - offset_beta(p={self.period},n={self.series_order})\",\n",
    "                mu=0,\n",
    "                sigma=1,\n",
    "                shape=(n_groups, 2 * self.series_order),\n",
    "            )\n",
    "\n",
    "            beta = pm.Deterministic(\n",
    "                f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                prev[\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                ]\n",
    "                + offset_beta * sigma_beta,\n",
    "            )\n",
    "\n",
    "        return pm.math.sum(x * beta[group], axis=1)\n",
    "\n",
    "    def _det_seasonality_posterior(self, beta, x):\n",
    "        return np.dot(x, beta.T)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                self._det_seasonality_posterior(\n",
    "                    map_approx[\n",
    "                        f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                    ][group_code],\n",
    "                    self._fourier_series(future),\n",
    "                )\n",
    "            )\n",
    "            future[f\"fs_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                self._det_seasonality_posterior(\n",
    "                    trace[\"posterior\"][\n",
    "                        f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                    ]\n",
    "                    .to_numpy()[:, :, group_code]\n",
    "                    .mean(0),\n",
    "                    self._fourier_series(future),\n",
    "                ).T.mean(0)\n",
    "            )\n",
    "            future[f\"fs_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        date = future[\"ds\"] if self.period > 7 else future[\"ds\"].dt.day_name()\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"fs_{self.model_idx} - p={self.period},n={self.series_order}\")\n",
    "        plt.grid()\n",
    "\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plt.plot(\n",
    "                date[-int(self.period) :],\n",
    "                future[f\"fs_{self.model_idx}_{group_code}\"][-int(self.period) :],\n",
    "                lw=1,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"FS(p={self.period},n={self.series_order},at={self.allow_tune},{self.pool_type})\"\n",
    "\n",
    "\n",
    "class Constant(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self, lower, upper, pool_cols=None, pool_type=\"complete\", allow_tune=False\n",
    "    ):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "        self.allow_tune = allow_tune\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"c\"] = model_idxs.get(\"c\", 0)\n",
    "        self.model_idx = model_idxs[\"c\"]\n",
    "        model_idxs[\"c\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                mu_c = pm.Uniform(\n",
    "                    f\"c_{self.model_idx} - mu_c(l={self.lower},u={self.upper})\",\n",
    "                    lower=self.lower,\n",
    "                    upper=self.upper,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "                offset_c = pm.Normal(\n",
    "                    f\"c_{self.model_idx} - offset_c(l={self.lower},u={self.upper})\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "                c = pm.Deterministic(\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\",\n",
    "                    mu_c + offset_c,\n",
    "                )\n",
    "            else:\n",
    "                c = pm.Uniform(\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\",\n",
    "                    lower=self.lower,\n",
    "                    upper=self.upper,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "\n",
    "        return c[group]\n",
    "\n",
    "    def _tune(self, model, data, initvals, model_idxs, prev):\n",
    "        return self.definition(model, data, initvals, model_idxs)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                np.ones_like(future[\"t\"])\n",
    "                * map_approx[f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\"][\n",
    "                    group_code\n",
    "                ]\n",
    "            )\n",
    "            future[f\"c_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                np.ones_like(future[\"t\"])\n",
    "                * trace[\"posterior\"][\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\"\n",
    "                ]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean()\n",
    "            )\n",
    "            future[f\"c_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\")\n",
    "\n",
    "        plot_data = []\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plot_data.append(\n",
    "                (group_name, future[f\"c_{self.model_idx}_{group_code}\"][0])\n",
    "            )\n",
    "\n",
    "        plt.bar(*zip(*plot_data))\n",
    "        plt.axhline(0, c=\"k\", linewidth=3)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"C(l={self.lower},u={self.upper},at={self.allow_tune},{self.pool_type})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [\"^W5000\", \"^GSPC\", \"^IXIC\", \"^DJI\"]\n",
    "\n",
    "gspc_tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"AMZN\", \"FB\", \"TSLA\", \"GOOGL\", \"GOOG\", \"JNJ\", \"JPM\", \"V\",\n",
    "    \"PG\", \"UNH\", \"DIS\", \"NVDA\", \"MA\", \"HD\", \"PYPL\", \"VZ\", \"ADBE\", \"CMCSA\",\n",
    "    \"NFLX\", \"BAC\", \"KO\", \"MRK\", \"PEP\", \"T\", \"PFE\", \"INTC\", \"CRM\", \"WMT\", \"ABT\",\n",
    "    \"ABBV\", \"CSCO\", \"TMO\", \"NKE\", \"AVGO\", \"XOM\", \"QCOM\", \"COST\", \"ACN\", \"CVX\",\n",
    "    \"MCD\", \"MDT\", \"NEE\", \"TXN\", \"HON\", \"DHR\", \"UNP\", \"BMY\", \"LIN\", \"LLY\",\n",
    "    \"AMGN\", \"PM\", \"C\", \"SBUX\", \"WFC\", \"ORCL\", \"UPS\", \"LOW\", \"BA\", \"IBM\", \"AMD\",\n",
    "    \"RTX\", \"NOW\", \"BLK\", \"MMM\", \"INTU\", \"AMT\", \"CAT\", \"MS\", \"CHTR\", \"ISRG\",\n",
    "    \"GE\", \"BKNG\", \"GS\", \"CVS\", \"TGT\", \"FIS\", \"LMT\", \"DE\", \"MU\", \"MDLZ\", \"TJX\",\n",
    "    \"SYK\", \"ANTM\", \"SCHW\", \"SPGI\", \"AXP\", \"AMAT\", \"TMUS\", \"ZTS\", \"MO\", \"ADP\",\n",
    "    \"CI\", \"PLD\", \"CL\", \"GILD\", \"BDX\", \"ATVI\", \"CB\", \"CSX\", \"CCI\", \"LRCX\",\n",
    "    \"DUK\", \"ADSK\", \"FISV\", \"CME\", \"SO\", \"ICE\", \"TFC\", \"GPN\", \"USB\", \"EQIX\",\n",
    "    \"PNC\", \"FDX\", \"VRTX\", \"D\", \"APD\", \"NSC\", \"EL\", \"SHW\", \"MMC\", \"ITW\", \"PGR\",\n",
    "    \"EW\", \"ADI\", \"HUM\", \"ILMN\", \"ECL\", \"GM\", \"DD\", \"DG\", \"BSX\", \"REGN\", \"AON\",\n",
    "    \"NEM\", \"EMR\", \"ETN\", \"NOC\", \"MCO\", \"KMB\", \"WM\", \"COF\", \"ROP\", \"CTSH\",\n",
    "    \"ROST\", \"HCA\", \"TWTR\", \"COP\", \"IDXX\", \"EA\", \"AEP\", \"EXC\", \"DOW\", \"BAX\",\n",
    "    \"TEL\", \"KLAC\", \"LHX\", \"SNPS\", \"APH\", \"DLR\", \"CMG\", \"ALGN\", \"CDNS\", \"SYY\",\n",
    "    \"FCX\", \"BIIB\", \"STZ\", \"MSCI\", \"SRE\", \"A\", \"MCHP\", \"GIS\", \"MET\", \"TRV\",\n",
    "    \"DXCM\", \"APTV\", \"PSA\", \"PH\", \"MAR\", \"XEL\", \"TT\", \"CNC\", \"XLNX\", \"GD\", \"BK\",\n",
    "    \"F\", \"IQV\", \"TROW\", \"ALXN\", \"MNST\", \"PPG\", \"HPQ\", \"VRSK\", \"JCI\", \"TDG\",\n",
    "    \"CMI\", \"INFO\", \"ALL\", \"EBAY\", \"ORLY\", \"YUM\", \"AIG\", \"ZBH\", \"SBAC\", \"ANSS\",\n",
    "    \"CTAS\", \"PRU\", \"HLT\", \"RMD\", \"CARR\", \"PSX\", \"BLL\", \"SLB\", \"PCAR\", \"PAYX\",\n",
    "    \"ES\", \"PEG\", \"ROK\", \"EOG\", \"AFL\", \"WEC\", \"CTVA\", \"MSI\", \"WBA\", \"SWK\",\n",
    "    \"ADM\", \"FAST\", \"SPG\", \"MCK\", \"AME\", \"AWK\", \"DFS\", \"LUV\", \"OTIS\", \"GLW\",\n",
    "    \"AZO\", \"VFC\", \"WLTW\", \"MTD\", \"WELL\", \"MPC\", \"KMI\", \"CPRT\", \"STT\", \"DAL\",\n",
    "    \"FRC\", \"CLX\", \"DLTR\", \"SWKS\", \"WY\", \"ED\", \"KR\", \"KEYS\", \"WMB\", \"CERN\",\n",
    "    \"TTWO\", \"FTV\", \"AJG\", \"EIX\", \"MKC\", \"MXIM\", \"LYB\", \"DTE\", \"EFX\", \"VLO\",\n",
    "    \"BBY\", \"AMP\", \"DHI\", \"FLT\", \"VTRS\", \"HSY\", \"KHC\", \"AVB\", \"PAYC\", \"ETSY\",\n",
    "    \"O\", \"VRSN\", \"PPL\", \"CHD\", \"MKTX\", \"ARE\", \"VIAC\", \"CBRE\", \"LEN\", \"WST\",\n",
    "    \"ZBRA\", \"EQR\", \"RSG\", \"SIVB\", \"FTNT\", \"ETR\", \"TER\", \"LH\", \"VMC\", \"FITB\",\n",
    "    \"LVS\", \"IP\", \"NTRS\", \"AEE\", \"TFX\", \"KSU\", \"QRVO\", \"TSN\", \"SYF\", \"CDW\",\n",
    "    \"ODFL\", \"PXD\", \"HOLX\", \"AMCR\", \"GWW\", \"VTR\", \"XYL\", \"DOV\", \"EXPE\", \"GRMN\",\n",
    "    \"COO\", \"CAG\", \"BR\", \"MLM\", \"TYL\", \"HIG\", \"CMS\", \"CTLT\", \"AKAM\", \"OKE\",\n",
    "    \"IR\", \"WDC\", \"URI\", \"HAL\", \"FE\", \"TSCO\", \"MTB\", \"PEAK\", \"INCY\", \"ULTA\",\n",
    "    \"STE\", \"CCL\", \"EXPD\", \"PKI\", \"NUE\", \"DGX\", \"KEY\", \"CTXS\", \"VAR\", \"K\",\n",
    "    \"ANET\", \"CAH\", \"ALB\", \"AES\", \"DRI\", \"KMX\", \"RF\", \"ESS\", \"WAT\", \"CFG\",\n",
    "    \"HPE\", \"NDAQ\", \"CE\", \"DPZ\", \"IEX\", \"EXR\", \"POOL\", \"FMC\", \"DRE\", \"NTAP\",\n",
    "    \"ABMD\", \"OXY\", \"MAA\", \"GPC\", \"TDY\", \"HES\", \"ABC\", \"MAS\", \"IT\", \"NVR\",\n",
    "    \"TIF\", \"J\", \"LDOS\", \"BKR\", \"STX\", \"RCL\", \"EMN\", \"OMC\", \"BXP\", \"SJM\", \"WAB\",\n",
    "    \"HRL\", \"PKG\", \"CINF\", \"AVY\", \"MGM\", \"LNT\", \"HBAN\", \"CHRW\", \"PFG\", \"UAL\",\n",
    "    \"EVRG\", \"BIO\", \"JKHY\", \"NLOK\", \"HAS\", \"ATO\", \"FBHS\", \"CNP\", \"RJF\", \"IFF\",\n",
    "    \"PHM\", \"LW\", \"CXO\", \"XRAY\", \"WRK\", \"JBHT\", \"UDR\", \"WHR\", \"HWM\", \"TXT\",\n",
    "    \"WYNN\", \"FFIV\", \"ALLE\", \"AAP\", \"UHS\", \"L\", \"LYV\", \"HST\", \"CBOE\", \"PWR\",\n",
    "    \"LKQ\", \"FOXA\", \"CPB\", \"AAL\", \"LUMN\", \"HSIC\", \"BWA\", \"RE\", \"WRB\", \"SNA\",\n",
    "    \"IPG\", \"NRG\", \"GL\", \"LNC\", \"WU\", \"PNW\", \"PNR\", \"NI\", \"LB\", \"DVA\", \"ROL\",\n",
    "    \"TPR\", \"TAP\", \"IRM\", \"MHK\", \"CF\", \"AIZ\", \"NCLH\", \"NWL\", \"DISH\", \"IPGP\",\n",
    "    \"MOS\", \"CMA\", \"DISCK\", \"FANG\", \"NLSN\", \"AOS\", \"JNPR\", \"REG\", \"ZION\", \"RHI\",\n",
    "    \"SEE\", \"NWSA\", \"HII\", \"BEN\", \"PVH\", \"IVZ\", \"DXC\", \"COG\", \"KIM\", \"ALK\",\n",
    "    \"PRGO\", \"DVN\", \"LEG\", \"FRT\", \"VNO\", \"FLIR\", \"PBCT\", \"APA\", \"NOV\", \"MRO\",\n",
    "    \"HBI\", \"RL\", \"DISCA\", \"FLS\", \"UNM\", \"VNT\", \"FOX\", \"SLG\", \"GPS\", \"FTI\",\n",
    "    \"XRX\", \"HFC\", \"UAA\", \"UA\", \"NWS\"\n",
    "]\n",
    "\n",
    "dji_tickers = [\n",
    "    \"DIS\", \"WMT\", \"DOW\", \"NKE\", \"CRM\", \"HD\", \"V\", \"MSFT\", \"MMM\", \"CSCO\", \"KO\",\n",
    "    \"AAPL\", \"HON\", \"JNJ\", \"TRV\", \"PG\", \"CVX\", \"VZ\", \"CAT\", \"BA\", \"AMGN\", \"IBM\",\n",
    "    \"AXP\", \"JPM\", \"WBA\", \"MCD\", \"MRK\", \"GS\", \"UNH\", \"INTC\"\n",
    "]\n",
    "\n",
    "ixic_tickers = [\n",
    "    \"FEYE\", \"ATEC\", \"SLAB\", \"CMRX\", \"NVCR\", \"FNLC\", \"NMRK\", \"SCOR\", \"AGLE\",\n",
    "    \"FARO\", \"OLMA\", \"TSLA\", \"FRTA\", \"AKTX\", \"KLXE\", \"CVCO\", \"NVCN\", \"EXAS\",\n",
    "    \"SDC\", \"BBQ\", \"IFRX\", \"CIIC\", \"BBI\", \"FNKO\", \"TWST\", \"FARM\", \"ACCD\",\n",
    "    \"NMRD\", \"FRSX\", \"OPTT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(tickers, start=\"1970-01-01\", end=\"2020-01-01\"):\n",
    "    data = yfinance.download(\n",
    "        tickers,\n",
    "        interval=\"1d\",\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "    downloaded_tickers = {col[1] for col in data.columns}\n",
    "    dfs = []\n",
    "    for ticker in downloaded_tickers:\n",
    "        df = pd.DataFrame(\n",
    "            data={\n",
    "                \"open\": data[\"Open\"][ticker].to_numpy(),\n",
    "                \"high\": data[\"High\"][ticker].to_numpy(),\n",
    "                \"low\": data[\"Low\"][ticker].to_numpy(),\n",
    "                \"close\": data[\"Close\"][ticker].to_numpy(),\n",
    "                \"typical_price\": (\n",
    "                    (\n",
    "                        data[\"Open\"][ticker]\n",
    "                        + data[\"High\"][ticker]\n",
    "                        + data[\"Low\"][ticker]\n",
    "                        + data[\"Close\"][ticker]\n",
    "                    )\n",
    "                    / 4\n",
    "                ).to_numpy(),\n",
    "                \"volume\": data[\"Volume\"][ticker].to_numpy(),\n",
    "            },\n",
    "            index=data[\"Close\"][ticker].index,\n",
    "        )\n",
    "\n",
    "        full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"D\")\n",
    "        df = df.reindex(full_date_range).interpolate()\n",
    "        df[\"ds\"] = df.index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df[\"series\"] = ticker\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_df(\n",
    "    start,\n",
    "    window,\n",
    "    horizon,\n",
    "    dfs,\n",
    "    for_prophet=False,\n",
    "    y_col=\"typical_price\",\n",
    "    perform_scaling=True,\n",
    "):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    scales = []\n",
    "    for df in dfs:\n",
    "        train_df = df[start : start + window].copy()\n",
    "        test_df = df[start + window : start + window + horizon].copy()\n",
    "        if train_df.isna().any().any() or test_df.isna().any().any():\n",
    "            continue\n",
    "\n",
    "        train_df[\"y\"] = train_df[y_col]\n",
    "        test_df[\"y\"] = test_df[y_col]\n",
    "\n",
    "        if perform_scaling:\n",
    "            scales.append(train_df[y_col].max())\n",
    "            train_df[\"y\"] = train_df[y_col] / scales[-1]\n",
    "            test_df[\"y\"] = test_df[y_col] / scales[-1]\n",
    "\n",
    "        train_dfs.append(train_df)\n",
    "        test_dfs.append(test_df)\n",
    "\n",
    "    if len(train_dfs) == 0:\n",
    "        return None\n",
    "\n",
    "    if for_prophet:\n",
    "        return train_dfs, test_dfs, scales\n",
    "\n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs), scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_df_around_point(\n",
    "    window,\n",
    "    horizon,\n",
    "    dfs,\n",
    "    point=\"2009-09-01\",\n",
    "    for_prophet=False,\n",
    "    y_col=\"typical_price\",\n",
    "    perform_scaling=True,\n",
    "):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    scales = []\n",
    "\n",
    "    for df in dfs:\n",
    "        point_idx = len(df[df[\"ds\"] < point])\n",
    "        check = generate_train_test_df(\n",
    "            start=point_idx - window,\n",
    "            window=window,\n",
    "            horizon=horizon,\n",
    "            dfs=[df],\n",
    "            for_prophet=for_prophet,\n",
    "            y_col=y_col,\n",
    "            perform_scaling=perform_scaling,\n",
    "        )\n",
    "        if check is None:\n",
    "            continue\n",
    "\n",
    "        train_df, test_df, scale = check\n",
    "\n",
    "        scales += scale\n",
    "\n",
    "        if for_prophet:\n",
    "            train_dfs += train_df\n",
    "            test_dfs += test_df\n",
    "        else:\n",
    "            train_dfs.append(train_df)\n",
    "            test_dfs.append(test_df)\n",
    "\n",
    "    if len(train_dfs) == 0:\n",
    "        return None\n",
    "    \n",
    "    if for_prophet:\n",
    "        return train_dfs, test_dfs, scales\n",
    "    \n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs), scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_components = [\n",
    "#     [LinearTrend(pool_cols=\"series\", pool_type=pt) for pt in [\"individual\", \"partial\"]],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=365.25, series_order=10, pool_cols=\"series\", pool_type=pt\n",
    "#         )\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=91.3125, series_order=n, pool_cols=\"series\", pool_type=pt\n",
    "#         )\n",
    "#         for n in range(7, 10)\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=30.4375, series_order=n, pool_cols=\"series\", pool_type=pt\n",
    "#         )\n",
    "#         for n in range(4, 7)\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "#     [\n",
    "#         FourierSeasonality(period=7, series_order=3, pool_cols=\"series\", pool_type=pt)\n",
    "#         for pt in [\"individual\", \"partial\"]\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "model_components = [\n",
    "    [LinearTrend(allow_tune=False)],\n",
    "    [\n",
    "        FourierSeasonality(period=365.25, series_order=10, allow_tune=allow_tune)\n",
    "        for allow_tune in [True]\n",
    "    ],\n",
    "    # [\n",
    "    #     FourierSeasonality(period=91.3125, series_order=n, allow_tune=allow_tune)\n",
    "    #     for n in range(7, 10)\n",
    "    #     for allow_tune in [True, False]\n",
    "    # ],\n",
    "    # [\n",
    "    #     FourierSeasonality(period=30.4375, series_order=n, allow_tune=allow_tune)\n",
    "    #     for n in range(4, 7)\n",
    "    #     for allow_tune in [True, False]\n",
    "    # ],\n",
    "    [\n",
    "        FourierSeasonality(period=7, series_order=3, allow_tune=allow_tune)\n",
    "        for allow_tune in [True, False]\n",
    "    ],\n",
    "]\n",
    "\n",
    "# model_components = [\n",
    "#     [LinearTrend(allow_tune=False, pool_cols=\"series\", pool_type=\"individual\")],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=365.25,\n",
    "#             series_order=10,\n",
    "#             allow_tune=allow_tune,\n",
    "#             pool_cols=\"series\",\n",
    "#             pool_type=\"individual\",\n",
    "#         )\n",
    "#         for allow_tune in [True]\n",
    "#     ],\n",
    "#     # [\n",
    "#     #     FourierSeasonality(\n",
    "#     #         period=91.3125,\n",
    "#     #         series_order=n,\n",
    "#     #         allow_tune=allow_tune,\n",
    "#     #         pool_cols=\"series\",\n",
    "#     #         pool_type=\"individual\",\n",
    "#     #     )\n",
    "#     #     for n in range(7, 10)\n",
    "#     #     for allow_tune in [True, False]\n",
    "#     # ],\n",
    "#     # [\n",
    "#     #     FourierSeasonality(\n",
    "#     #         period=30.4375,\n",
    "#     #         series_order=n,\n",
    "#     #         allow_tune=allow_tune,\n",
    "#     #         pool_cols=\"series\",\n",
    "#     #         pool_type=\"individual\",\n",
    "#     #     )\n",
    "#     #     for n in range(4, 7)\n",
    "#     #     for allow_tune in [True, False]\n",
    "#     # ],\n",
    "#     [\n",
    "#         FourierSeasonality(\n",
    "#             period=7,\n",
    "#             series_order=3,\n",
    "#             allow_tune=allow_tune,\n",
    "#             pool_cols=\"series\",\n",
    "#             pool_type=\"individual\",\n",
    "#         )\n",
    "#         for allow_tune in [True, False]\n",
    "#     ],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [(0, [mc]) for mc in model_components[0]]\n",
    "models = []\n",
    "\n",
    "while len(q):\n",
    "    level, model = q.pop(0)\n",
    "    if level + 1 == len(model_components):\n",
    "        models.append(model)\n",
    "        continue\n",
    "\n",
    "    mcs = model_components[level + 1]\n",
    "    for mc in mcs:\n",
    "        # if mc.pool_type == \"partial\":\n",
    "        #     q.append(\n",
    "        #         (\n",
    "        #             level + 1,\n",
    "        #             model\n",
    "        #             + [\n",
    "        #                 Constant(\n",
    "        #                     lower=-1, upper=1, pool_cols=\"series\", pool_type=\"partial\"\n",
    "        #                 )\n",
    "        #                 * mc\n",
    "        #             ],\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        q.append((level + 1, model + [mc]))\n",
    "        q.append((level + 1, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_models(models):\n",
    "    s = None\n",
    "    for model in models:\n",
    "        if s is None:\n",
    "            s = model\n",
    "        else:\n",
    "            s += model\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (\n",
    "    [\n",
    "        LinearTrend() * (FourierSeasonality(365.25, 10) + FourierSeasonality(7, 3)),\n",
    "        LinearTrend() * FourierSeasonality(365.25, 10),\n",
    "        LinearTrend() * FourierSeasonality(7, 3),\n",
    "        LinearTrend() + FourierSeasonality(365.25, 10) + FourierSeasonality(7, 3),\n",
    "        LinearTrend() + FourierSeasonality(365.25, 10),\n",
    "        LinearTrend() + FourierSeasonality(7, 3),\n",
    "    ]\n",
    "    + [\n",
    "        model[0] * sum_models(model[1:]) if len(model) > 1 else model[0]\n",
    "        for model in models\n",
    "    ]\n",
    "    + [sum_models(model) if len(model) > 1 else model[0] for model in models]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_models = {\"\"}\n",
    "final_models = []\n",
    "for model in models:\n",
    "    str_model = str(model)\n",
    "    if str_model in str_models:\n",
    "        continue\n",
    "\n",
    "    str_models.add(str_model)\n",
    "    final_models.append(model)\n",
    "\n",
    "len(final_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'LT(n=25,r=0.8,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=False,complete) + FS(p=7,n=3,at=False,complete))',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=True,complete) + FS(p=7,n=3,at=False,complete))',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=True,complete) + FS(p=7,n=3,at=True,complete))',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * FS(p=365.25,n=10,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * FS(p=365.25,n=10,at=True,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * FS(p=7,n=3,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) * FS(p=7,n=3,at=True,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=365.25,n=10,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=365.25,n=10,at=False,complete) + FS(p=7,n=3,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=365.25,n=10,at=True,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=365.25,n=10,at=True,complete) + FS(p=7,n=3,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=365.25,n=10,at=True,complete) + FS(p=7,n=3,at=True,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=7,n=3,at=False,complete)',\n",
       " 'LT(n=25,r=0.8,at=False,complete) + FS(p=7,n=3,at=True,complete)'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prophet_metrics(y_trues, y_preds, horizon):\n",
    "    result = None\n",
    "    for y_true, y_pred in zip(y_trues, y_preds):\n",
    "        group_name = y_true[\"series\"].iloc[0]\n",
    "        single_metrics = {\"mse\": {}, \"rmse\": {}, \"mae\": {}, \"mape\": {}}\n",
    "        single_metrics[\"mse\"][group_name] = mean_squared_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        single_metrics[\"rmse\"][group_name] = root_mean_squared_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        single_metrics[\"mae\"][group_name] = mean_absolute_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        single_metrics[\"mape\"][group_name] = mean_absolute_percentage_error(\n",
    "            y_true[\"y\"], y_pred[\"yhat\"][-horizon:]\n",
    "        )\n",
    "        if result is None:\n",
    "            result = pd.DataFrame(single_metrics)\n",
    "        else:\n",
    "            result = pd.concat((result, pd.DataFrame(single_metrics)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  503 of 503 completed\n",
      "\n",
      "83 Failed downloads:\n",
      "['NWSA', 'PAYC', 'FOX', 'PYPL', 'ZTS', 'KMI', 'ANET', 'CARR', 'HLT', 'INFO', 'FOXA', 'SYF', 'HCA', 'CFG', 'IR', 'AMCR', 'ETSY', 'IQV', 'LW', 'KEYS', 'CDW', 'NCLH', 'PSX', 'HPE', 'FTV', 'ALLE', 'LB', 'UA', 'APTV', 'QRVO', 'FANG', 'HII', 'OTIS', 'CTVA', 'NWS', 'XYL', 'NOW', 'MPC', 'ABBV', 'FISV', 'VNT', 'CTLT', 'HWM', 'DOW', 'KHC']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2000-01-01 -> 2011-01-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 946702800, endDate = 1293858000\")')\n",
      "['ABC', 'GPS', 'VIAC', 'FBHS', 'FLT', 'PXD', 'TIF', 'DISH', 'KSU', 'CTXS', 'CXO', 'RE', 'COG', 'ATVI', 'TWTR', 'FLIR', 'ABMD', 'VAR', 'BLL', 'WRK', 'ALXN', 'FB', 'DRE', 'XLNX', 'HFC', 'WLTW', 'FRC', 'PBCT', 'ANTM', 'NLSN', 'PEAK', 'MXIM', 'PKI', 'NLOK', 'DISCA', 'DISCK', 'CERN', 'SIVB']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    }
   ],
   "source": [
    "smp = fetch_data([\"^GSPC\"])\n",
    "smp_tickers = fetch_data(gspc_tickers, start=\"2000-01-01\", end=\"2011-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>typical_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>ds</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.540001</td>\n",
       "      <td>91.790001</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>69.582500</td>\n",
       "      <td>8.050000e+06</td>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.776667</td>\n",
       "      <td>92.036667</td>\n",
       "      <td>93.153333</td>\n",
       "      <td>69.741667</td>\n",
       "      <td>9.196667e+06</td>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.013334</td>\n",
       "      <td>92.283333</td>\n",
       "      <td>93.306666</td>\n",
       "      <td>69.900833</td>\n",
       "      <td>1.034333e+07</td>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>92.529999</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>70.059999</td>\n",
       "      <td>1.149000e+07</td>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.809998</td>\n",
       "      <td>92.129997</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>69.689999</td>\n",
       "      <td>1.146000e+07</td>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18256</th>\n",
       "      <td>3247.229980</td>\n",
       "      <td>3247.929932</td>\n",
       "      <td>3234.370117</td>\n",
       "      <td>3240.020020</td>\n",
       "      <td>3242.387512</td>\n",
       "      <td>2.429150e+09</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18257</th>\n",
       "      <td>3244.850016</td>\n",
       "      <td>3245.593262</td>\n",
       "      <td>3228.436768</td>\n",
       "      <td>3233.776693</td>\n",
       "      <td>3238.164185</td>\n",
       "      <td>2.626673e+09</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18258</th>\n",
       "      <td>3242.470052</td>\n",
       "      <td>3243.256592</td>\n",
       "      <td>3222.503418</td>\n",
       "      <td>3227.533366</td>\n",
       "      <td>3233.940857</td>\n",
       "      <td>2.824197e+09</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18259</th>\n",
       "      <td>3240.090088</td>\n",
       "      <td>3240.919922</td>\n",
       "      <td>3216.570068</td>\n",
       "      <td>3221.290039</td>\n",
       "      <td>3229.717529</td>\n",
       "      <td>3.021720e+09</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18260</th>\n",
       "      <td>3215.179932</td>\n",
       "      <td>3231.719971</td>\n",
       "      <td>3212.030029</td>\n",
       "      <td>3230.780029</td>\n",
       "      <td>3222.427490</td>\n",
       "      <td>2.894760e+09</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>^GSPC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18261 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              open         high          low        close  typical_price  \\\n",
       "0         0.000000    93.540001    91.790001    93.000000      69.582500   \n",
       "1         0.000000    93.776667    92.036667    93.153333      69.741667   \n",
       "2         0.000000    94.013334    92.283333    93.306666      69.900833   \n",
       "3         0.000000    94.250000    92.529999    93.459999      70.059999   \n",
       "4         0.000000    93.809998    92.129997    92.820000      69.689999   \n",
       "...            ...          ...          ...          ...            ...   \n",
       "18256  3247.229980  3247.929932  3234.370117  3240.020020    3242.387512   \n",
       "18257  3244.850016  3245.593262  3228.436768  3233.776693    3238.164185   \n",
       "18258  3242.470052  3243.256592  3222.503418  3227.533366    3233.940857   \n",
       "18259  3240.090088  3240.919922  3216.570068  3221.290039    3229.717529   \n",
       "18260  3215.179932  3231.719971  3212.030029  3230.780029    3222.427490   \n",
       "\n",
       "             volume         ds series  \n",
       "0      8.050000e+06 1970-01-02  ^GSPC  \n",
       "1      9.196667e+06 1970-01-03  ^GSPC  \n",
       "2      1.034333e+07 1970-01-04  ^GSPC  \n",
       "3      1.149000e+07 1970-01-05  ^GSPC  \n",
       "4      1.146000e+07 1970-01-06  ^GSPC  \n",
       "...             ...        ...    ...  \n",
       "18256  2.429150e+09 2019-12-27  ^GSPC  \n",
       "18257  2.626673e+09 2019-12-28  ^GSPC  \n",
       "18258  2.824197e+09 2019-12-29  ^GSPC  \n",
       "18259  3.021720e+09 2019-12-30  ^GSPC  \n",
       "18260  2.894760e+09 2019-12-31  ^GSPC  \n",
       "\n",
       "[18261 rows x 8 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crisis_metrics.index.to_list()\n",
    "crisis_tickers = smp + [t for t in smp_tickers if t[\"series\"].iloc[0] in crisis_metrics.index]\n",
    "len(crisis_tickers), len(crisis_metrics.index.to_list())\n",
    "crisis_tickers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = \"2006-01-01\"\n",
    "\n",
    "train_df_smp, test_df_smp, scales_smp = generate_train_test_df_around_point(\n",
    "    window=365 * 35, horizon=365, dfs=smp, for_prophet=True, point=point\n",
    ")\n",
    "train_df_ticker, test_df_tickers, scales_tickers = generate_train_test_df_around_point(\n",
    "    window=365 * 1, horizon=365, dfs=smp + smp_tickers, for_prophet=True, point=point\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smp_tickers[::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>0.034712</td>\n",
       "      <td>0.186312</td>\n",
       "      <td>0.181553</td>\n",
       "      <td>0.210649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse      rmse       mae      mape\n",
       "^GSPC  0.034712  0.186312  0.181553  0.210649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_prophet = Prophet(seasonality_mode=\"multiplicative\")\n",
    "context_prophet.fit(train_df_smp[0])\n",
    "context_future = context_prophet.make_future_dataframe(\n",
    "    periods=365, include_history=True\n",
    ")\n",
    "context_yhat = context_prophet.predict(context_future)\n",
    "context_metrics = get_prophet_metrics(test_df_smp, [context_yhat], 365)\n",
    "context_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "385it [01:36,  4.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15101921375067284"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_forecasts = []\n",
    "\n",
    "for df, df_test in tqdm(zip(train_df_ticker, test_df_tickers)):\n",
    "    prophet = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True)\n",
    "    # prophet.add_regressor(\"smp_weekly\", standardize=False, mode=\"additive\")\n",
    "    # prophet.add_regressor(\"smp_yearly\", standardize=False, mode=\"additive\")\n",
    "    # prophet.add_regressor(\"smp_yhat\", standardize=False, mode=\"additive\")\n",
    "\n",
    "    train_df = df.copy()\n",
    "    # train_df[\"smp_weekly\"] = context_yhat[\n",
    "    #     (context_yhat[\"ds\"] >= train_df[\"ds\"].iloc[0])\n",
    "    #     & (context_yhat[\"ds\"] <= train_df[\"ds\"].iloc[-1])\n",
    "    # ][\"weekly\"].to_numpy()\n",
    "    # train_df[\"smp_yhat\"] = context_yhat[\n",
    "    #     (context_yhat[\"ds\"] >= train_df[\"ds\"].iloc[0])\n",
    "    #     & (context_yhat[\"ds\"] <= train_df[\"ds\"].iloc[-1])\n",
    "    # ][\"yhat\"].to_numpy()\n",
    "    # train_df[\"smp_yearly\"] = context_yhat[\n",
    "    #     (context_yhat[\"ds\"] >= train_df[\"ds\"].iloc[0])\n",
    "    #     & (context_yhat[\"ds\"] <= train_df[\"ds\"].iloc[-1])\n",
    "    # ][\"yearly\"].to_numpy()\n",
    "\n",
    "    prophet.fit(train_df)\n",
    "\n",
    "    future = prophet.make_future_dataframe(periods=365, include_history=True)\n",
    "    # future[\"smp_weekly\"] = context_yhat[\n",
    "    #     (context_yhat[\"ds\"] >= future[\"ds\"].iloc[0])\n",
    "    #     & (context_yhat[\"ds\"] <= future[\"ds\"].iloc[-1])\n",
    "    # ][\"weekly\"].to_numpy()\n",
    "    # future[\"smp_yhat\"] = context_yhat[\n",
    "    #     (context_yhat[\"ds\"] >= future[\"ds\"].iloc[0])\n",
    "    #     & (context_yhat[\"ds\"] <= future[\"ds\"].iloc[-1])\n",
    "    # ][\"yhat\"].to_numpy()\n",
    "    # future[\"smp_yearly\"] = context_yhat[\n",
    "    #     (context_yhat[\"ds\"] >= future[\"ds\"].iloc[0])\n",
    "    #     & (context_yhat[\"ds\"] <= future[\"ds\"].iloc[-1])\n",
    "    # ][\"yearly\"].to_numpy()\n",
    "\n",
    "    prophet_forecasts.append(prophet.predict(future))\n",
    "\n",
    "prophet_metrics = get_prophet_metrics(test_df_tickers, prophet_forecasts, 365)\n",
    "prophet_metrics[\"mape\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.174264</td>\n",
       "      <td>0.156164</td>\n",
       "      <td>0.150546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KR</th>\n",
       "      <td>0.070413</td>\n",
       "      <td>0.265355</td>\n",
       "      <td>0.226613</td>\n",
       "      <td>0.209672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TYL</th>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.112678</td>\n",
       "      <td>0.089243</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WAB</th>\n",
       "      <td>0.035217</td>\n",
       "      <td>0.187662</td>\n",
       "      <td>0.154660</td>\n",
       "      <td>0.130060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTSH</th>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.072825</td>\n",
       "      <td>0.060088</td>\n",
       "      <td>0.047373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD</th>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.181503</td>\n",
       "      <td>0.166658</td>\n",
       "      <td>0.227767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP</th>\n",
       "      <td>0.300409</td>\n",
       "      <td>0.548096</td>\n",
       "      <td>0.482266</td>\n",
       "      <td>0.571892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCI</th>\n",
       "      <td>0.014618</td>\n",
       "      <td>0.120904</td>\n",
       "      <td>0.093315</td>\n",
       "      <td>0.079542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.077427</td>\n",
       "      <td>0.278258</td>\n",
       "      <td>0.249598</td>\n",
       "      <td>0.234854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AES</th>\n",
       "      <td>0.035449</td>\n",
       "      <td>0.188278</td>\n",
       "      <td>0.156029</td>\n",
       "      <td>0.135403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse      rmse       mae      mape\n",
       "^GSPC  0.030368  0.174264  0.156164  0.150546\n",
       "KR     0.070413  0.265355  0.226613  0.209672\n",
       "TYL    0.012696  0.112678  0.089243  0.064205\n",
       "WAB    0.035217  0.187662  0.154660  0.130060\n",
       "CTSH   0.005304  0.072825  0.060088  0.047373\n",
       "...         ...       ...       ...       ...\n",
       "DD     0.032943  0.181503  0.166658  0.227767\n",
       "IP     0.300409  0.548096  0.482266  0.571892\n",
       "CCI    0.014618  0.120904  0.093315  0.079542\n",
       "RF     0.077427  0.278258  0.249598  0.234854\n",
       "AES    0.035449  0.188278  0.156029  0.135403\n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(idx, point):\n",
    "    train_df_smp, test_df_smp, scales_smp = generate_train_test_df_around_point(\n",
    "        window=365 * 35, horizon=365, dfs=smp, for_prophet=False, point=point\n",
    "    )\n",
    "    model = final_models[idx]\n",
    "    model.fit(train_df_smp, progressbar=False)\n",
    "    map_approx = model.map_approx\n",
    "    model_metrics = []\n",
    "\n",
    "    for smp_ticker in tqdm(smp + smp_tickers):\n",
    "        check = generate_train_test_df_around_point(\n",
    "            window=365 * 1,\n",
    "            horizon=365,\n",
    "            dfs=[smp_ticker],\n",
    "            for_prophet=False,\n",
    "            point=point,\n",
    "        )\n",
    "        if check is None:\n",
    "            continue\n",
    "\n",
    "        train_df_tickers, test_df_tickers, scales_tickers = check\n",
    "        model.map_approx = map_approx\n",
    "        model.tune(train_df_tickers, progressbar=False)\n",
    "        yhat = model.predict(365)\n",
    "        yhat.to_csv(f\"./out/model_{idx}_series_{smp_ticker['series'].iloc[0]}.csv\")\n",
    "        model_metrics.append(model.metrics(test_df_tickers, yhat, pool_cols=\"series\"))\n",
    "\n",
    "    print(f\"{idx} - {model}: {pd.concat(model_metrics)['mape'].mean()}\")\n",
    "    return pd.concat(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "\"\"\"\n",
    "100%|██████████| 127/127 [09:50<00:00,  4.65s/it]\n",
    "0 - LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=True,complete) + FS(p=7,n=3,at=True,complete)): 0.15938053967032853\n",
    "100%|██████████| 127/127 [05:26<00:00,  2.57s/it]\n",
    "1 - LT(n=25,r=0.8,at=False,complete) * FS(p=365.25,n=10,at=True,complete): 0.15760077719458276\n",
    "100%|██████████| 127/127 [06:05<00:00,  2.88s/it]\n",
    "2 - LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=True,complete) + FS(p=7,n=3,at=False,complete)): 0.1613983544054207\n",
    "100%|██████████| 127/127 [05:36<00:00,  2.65s/it]\n",
    "3 - LT(n=25,r=0.8,at=False,complete) * FS(p=7,n=3,at=True,complete): 0.2135529074152472\n",
    "100%|██████████| 127/127 [03:38<00:00,  1.72s/it]\n",
    "4 - LT(n=25,r=0.8,at=False,complete): 0.2123689870169386\n",
    "100%|██████████| 127/127 [03:48<00:00,  1.80s/it]\n",
    "5 - LT(n=25,r=0.8,at=False,complete) * FS(p=7,n=3,at=False,complete): 0.21387403836166552\n",
    "\"\"\"\n",
    "\n",
    "point = \"2006-01-01\"\n",
    "all_metrics = []\n",
    "# all_metrics = Parallel(n_jobs=8, prefer=\"threads\")(\n",
    "#     delayed(run_test)(idx, point) for idx, model in enumerate(final_models[:5])\n",
    "# )\n",
    "\n",
    "# with open(\"./out/models.txt\", \"w\") as f:\n",
    "#     for idx, model in enumerate(final_models):\n",
    "#         f.write(f\"model_{idx}: {model}\\n\")\n",
    "\n",
    "for idx, model in enumerate(final_models):\n",
    "    all_metrics.append(run_test(idx, point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1594064715617123\n",
      "0.15959596825589623\n",
      "0.15933096056975335\n",
      "0.21868648369294974\n",
      "0.21866633274091107\n",
      "0.21860483207263137\n"
     ]
    }
   ],
   "source": [
    "for metrics in all_metrics:\n",
    "    print(metrics[\"mape\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11044587739696389\n",
      "[0, 1, 5, 5, 4, 2, 1, 2, 1, 3, 2, 4, 0, 5, 0, 4, 0, 3, 0, 1, 0, 3, 2, 4, 2, 1, 1, 3, 3, 0, 1, 0, 1, 1, 4, 1, 1, 0, 0, 1, 0, 5, 1, 5, 5, 0, 1, 4, 0, 5, 0, 5, 3, 3, 2, 1, 1, 3, 3, 1, 0, 4, 4, 1, 0, 2, 4, 0, 2, 0, 5, 1, 3, 4, 3, 1, 4, 0, 5, 0, 2, 5, 5, 5, 2, 2, 4, 1, 3, 0, 4, 0, 2, 5, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "mapes = [100 for _ in all_metrics[0].index]\n",
    "model_idxs = [0 for _ in all_metrics[0].index]\n",
    "\n",
    "for idx, ticker in enumerate(all_metrics[0].index):\n",
    "    for model_idx, metrics in enumerate(all_metrics):\n",
    "        if mapes[idx] > metrics.loc[ticker][\"mape\"]:\n",
    "            mapes[idx] = min(mapes[idx], metrics.loc[ticker][\"mape\"])\n",
    "            model_idxs[idx] = model_idx\n",
    "\n",
    "print(sum(mapes) / len(mapes))\n",
    "print(model_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [07:11,  4.50s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m train_df_tickers, test_df_tickers, scales_tickers \u001b[38;5;241m=\u001b[39m check\n\u001b[0;32m---> 17\u001b[0m model_idx \u001b[38;5;241m=\u001b[39m model_idxs[idx]\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m final_models[model_idx]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tested_models:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_df_smp, test_df_smp, scales_smp = generate_train_test_df_around_point(\n",
    "    window=365 * 35, horizon=365, dfs=smp, for_prophet=False\n",
    ")\n",
    "\n",
    "model_metrics = []\n",
    "tested_models = []\n",
    "\n",
    "for idx, smp_ticker in tqdm(enumerate(smp + smp_tickers[::4])):\n",
    "    check = generate_train_test_df_around_point(\n",
    "        window=365 * 1, horizon=365, dfs=[smp_ticker], for_prophet=False\n",
    "    )\n",
    "    if check is None:\n",
    "        continue\n",
    "\n",
    "    train_df_tickers, test_df_tickers, scales_tickers = check\n",
    "    \n",
    "    model_idx = model_idxs[idx]\n",
    "    model = final_models[model_idx]\n",
    "    if model_idx not in tested_models:\n",
    "        model.fit(train_df_smp, progressbar=False)\n",
    "        tested_models.append(model_idx)\n",
    "\n",
    "    map_approx = model.map_approx    \n",
    "    model.tune(train_df_tickers, progressbar=False)\n",
    "    yhat = model.predict(365)\n",
    "    model_metrics.append(model.metrics(test_df_tickers, yhat, pool_cols=\"series\"))\n",
    "    model.map_approx = map_approx\n",
    "\n",
    "print(f\"{pd.concat(model_metrics)['mape'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25708000472312514"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crisis_metrics = pd.concat(model_metrics)\n",
    "crisis_metrics[\"mape\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [03:51<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT(n=25,r=0.8,at=False,complete) * (FS(p=365.25,n=10,at=False,complete) + FS(p=7,n=3,at=False,complete)): 0.15500495141015147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LinearTrend() * (FourierSeasonality(365.25, 10) + FourierSeasonality(7, 3))\n",
    "model_metrics = []\n",
    "\n",
    "for smp_ticker in tqdm(smp + smp_tickers[::4]):\n",
    "    check = generate_train_test_df_around_point(\n",
    "        window=365 * 1,\n",
    "        horizon=365,\n",
    "        dfs=[smp_ticker],\n",
    "        for_prophet=False,\n",
    "        point=point,\n",
    "    )\n",
    "    if check is None:\n",
    "        continue\n",
    "\n",
    "    train_df_tickers, test_df_tickers, scales_tickers = check\n",
    "    model.fit(train_df_tickers, progressbar=False)\n",
    "    yhat = model.predict(365)\n",
    "    model_metrics.append(model.metrics(test_df_tickers, yhat, pool_cols=\"series\"))\n",
    "\n",
    "print(f\"{model}: {pd.concat(model_metrics)['mape'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_2(idx, point):\n",
    "    train_df_smp, test_df_smp, scales_smp = generate_train_test_df_around_point(\n",
    "        window=365 * 35, horizon=365, dfs=smp, for_prophet=False, point=point\n",
    "    )\n",
    "    model = final_models[idx]\n",
    "    model.fit(train_df_smp, progressbar=False)\n",
    "    map_approx = model.map_approx\n",
    "    model_metrics = []\n",
    "\n",
    "    check = generate_train_test_df_around_point(\n",
    "        window=365 * 1,\n",
    "        horizon=365,\n",
    "        dfs=smp + smp_tickers[::4],\n",
    "        for_prophet=False,\n",
    "        point=point,\n",
    "    )\n",
    "\n",
    "    train_df_tickers, test_df_tickers, scales_tickers = check\n",
    "    model.map_approx = map_approx\n",
    "    model.tune(train_df_tickers, progressbar=True)\n",
    "    yhat = model.predict(365)\n",
    "    yhat.to_csv(f\"./out/single_model_{idx}.csv\")\n",
    "    model_metrics = model.metrics(test_df_tickers, yhat, pool_cols=\"series\")\n",
    "\n",
    "    print(f\"{idx} - {model}: {model_metrics['mape'].mean()}\")\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
