{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "import yfinance\n",
    "\n",
    "logging.getLogger(\"prophet\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"cmdstanpy\").disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_definition(X, pool_cols, pool_type):\n",
    "    if pool_type == \"complete\":\n",
    "        group = np.zeros(len(X), dtype=\"int\")\n",
    "        group_mapping = {0: \"all\"}\n",
    "        n_groups = 1\n",
    "    else:\n",
    "        X[pool_cols] = pd.Categorical(X[pool_cols])\n",
    "        group = X[pool_cols].cat.codes.values\n",
    "        group_mapping = dict(enumerate(X[pool_cols].cat.categories))\n",
    "        n_groups = X[pool_cols].nunique()\n",
    "    return group, n_groups, group_mapping\n",
    "\n",
    "\n",
    "class TimeSeriesModel:\n",
    "    def _scale_data(self):\n",
    "        self.y_min = 0\n",
    "        self.y_max = self.data[\"y\"].abs().max()\n",
    "        self.ds_min = self.data[\"ds\"].min()\n",
    "        self.ds_max = self.data[\"ds\"].max()\n",
    "\n",
    "        self.data[\"y\"] = self.data[\"y\"] / self.y_max\n",
    "        self.data[\"t\"] = (self.data[\"ds\"] - self.ds_min) / (self.ds_max - self.ds_min)\n",
    "\n",
    "    def _process_data(self):\n",
    "        self.data[\"ds\"] = pd.to_datetime(self.data[\"ds\"])\n",
    "        self.data.sort_values(\"ds\", inplace=True)\n",
    "        self._scale_data()\n",
    "\n",
    "    def _model_init(self):\n",
    "        i0, i1 = self.data[\"ds\"].idxmin(), self.data[\"ds\"].idxmax()\n",
    "        T = self.data[\"t\"].iloc[i1] - self.data[\"t\"].iloc[i0]\n",
    "        slope = (self.data[\"y\"].iloc[i1] - self.data[\"y\"].iloc[i0]) / T\n",
    "        intercept = self.data[\"y\"].iloc[i0] - slope * self.data[\"t\"].iloc[i0]\n",
    "        return {\n",
    "            \"slope\": slope,\n",
    "            \"intercept\": intercept,\n",
    "            \"delta\": 0.0,\n",
    "            \"beta\": 0.0,\n",
    "            \"sigma\": 1.0,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        sigma_sd=0.5,\n",
    "        mcmc_samples=0,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        use_prophet_initvals=True,\n",
    "        progressbar=True,\n",
    "    ):\n",
    "        self.mcmc_samples = mcmc_samples\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self._process_data()\n",
    "\n",
    "        self.initvals = {}\n",
    "        # if use_prophet_initvals:\n",
    "        #     self.initvals = self._model_init()\n",
    "\n",
    "        self.model = pm.Model()\n",
    "        self.model_idxs = {}\n",
    "        mu = self.definition(self.model, self.data, self.initvals, self.model_idxs)\n",
    "\n",
    "        with self.model:\n",
    "            sigma = pm.HalfNormal(\n",
    "                \"sigma\", sigma_sd, initval=self.initvals.get(\"sigma\", 1)\n",
    "            )\n",
    "            _ = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=self.data[\"y\"])\n",
    "\n",
    "            self.map_approx = None\n",
    "            self.trace = None\n",
    "            if self.mcmc_samples == 0:\n",
    "                self.map_approx = pm.find_MAP(progressbar=progressbar, maxeval=1e4)\n",
    "            else:\n",
    "                self.trace = pm.sample(self.mcmc_samples, chains=chains, cores=cores)\n",
    "\n",
    "    def _make_future_df(self, days):\n",
    "        future = pd.DataFrame(\n",
    "            {\n",
    "                \"ds\": pd.DatetimeIndex(\n",
    "                    np.hstack(\n",
    "                        (\n",
    "                            self.data[\"ds\"].unique().to_numpy(),\n",
    "                            pd.date_range(\n",
    "                                self.ds_max,\n",
    "                                self.ds_max + pd.Timedelta(days, \"D\"),\n",
    "                                inclusive=\"right\",\n",
    "                            ).to_numpy(),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        future[\"t\"] = (future[\"ds\"] - self.ds_min) / (self.ds_max - self.ds_min)\n",
    "        return future\n",
    "\n",
    "    def predict(self, days):\n",
    "        future = self._make_future_df(days)\n",
    "        forecasts = self._predict(\n",
    "            future, self.mcmc_samples, self.map_approx, self.trace\n",
    "        )\n",
    "\n",
    "        for group_code in range(forecasts.shape[0]):\n",
    "            future[f\"yhat_{group_code}\"] = forecasts[group_code] * self.y_max\n",
    "            for model_type, model_cnt in self.model_idxs.items():\n",
    "                if model_type.startswith(\"fs\"):\n",
    "                    continue\n",
    "                for model_idx in range(model_cnt):\n",
    "                    component = f\"{model_type}_{model_idx}_{group_code}\"\n",
    "                    if component in future.columns:\n",
    "                        future[component] *= self.y_max\n",
    "\n",
    "        return future\n",
    "\n",
    "    def _predict(self, future, mcmc_samples, map_approx, trace):\n",
    "        if mcmc_samples == 0:\n",
    "            return self._predict_map(future, map_approx)\n",
    "\n",
    "        return self._predict_mcmc(future, trace)\n",
    "\n",
    "    def plot(self, future, y_true=None, pool_cols=None):\n",
    "        plt.figure(figsize=(14, 100 * 6))\n",
    "        plt.subplot(100, 1, 1)\n",
    "        plt.title(\"Predictions\")\n",
    "        plt.grid()\n",
    "\n",
    "        group, _, groups_ = get_group_definition(self.data, pool_cols, \"not_complete\")\n",
    "        for group_code, group_name in groups_.items():\n",
    "            group_idx = group == group_code\n",
    "            color = np.random.rand(3)\n",
    "            plt.scatter(\n",
    "                self.data[\"ds\"][group_idx],\n",
    "                self.data[\"y\"][group_idx] * self.y_max,\n",
    "                s=0.5,\n",
    "                color=color,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        if y_true is not None:\n",
    "            test_group, _, test_groups_ = get_group_definition(\n",
    "                y_true, pool_cols, \"not_complete\"\n",
    "            )\n",
    "            for group_code, group_name in test_groups_.items():\n",
    "                group_idx = test_group == group_code\n",
    "                color = np.random.rand(3)\n",
    "                plt.scatter(\n",
    "                    y_true[\"ds\"][group_idx],\n",
    "                    y_true[\"y\"][group_idx],\n",
    "                    s=0.5,\n",
    "                    color=color,\n",
    "                    label=f\"y - {group_name}\",\n",
    "                )\n",
    "\n",
    "        for group_code, group_name in groups_.items():\n",
    "            plt.plot(\n",
    "                future[\"ds\"],\n",
    "                future[f\"yhat_{group_code}\"],\n",
    "                lw=1,\n",
    "                label=f\"yhat - {group_name}\",\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "        plot_params = {\"idx\": 1}\n",
    "        self._plot(plot_params, future, self.data, self.y_max, y_true)\n",
    "\n",
    "    def metrics(self, y_true, future, pool_cols=None, pool_type=\"individual\"):\n",
    "        metrics = {\"mse\": {}, \"rmse\": {}, \"mae\": {}, \"mape\": {}}\n",
    "        test_group, _, test_groups_ = get_group_definition(y_true, pool_cols, pool_type)\n",
    "        for group_code, group_name in test_groups_.items():\n",
    "            group_idx = test_group == group_code\n",
    "            y = y_true[\"y\"][group_idx]\n",
    "            yhat = future[f\"yhat_{group_code}\"][-len(y) :]\n",
    "            metrics[\"mse\"][group_name] = mean_squared_error(y, yhat)\n",
    "            metrics[\"rmse\"][group_name] = root_mean_squared_error(y, yhat)\n",
    "            metrics[\"mae\"][group_name] = mean_absolute_error(y, yhat)\n",
    "            metrics[\"mape\"][group_name] = mean_absolute_percentage_error(y, yhat)\n",
    "\n",
    "        return pd.DataFrame(metrics)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return AdditiveTimeSeries(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MultiplicativeTimeSeries(self, other)\n",
    "\n",
    "\n",
    "class AdditiveTimeSeries(TimeSeriesModel):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def definition(self, *args, **kwargs):\n",
    "        return self.left.definition(*args, **kwargs) + self.right.definition(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def _predict(self, *args, **kwargs):\n",
    "        return self.left._predict(*args, **kwargs) + self.right._predict(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def _plot(self, *args, **kwargs):\n",
    "        self.left._plot(*args, **kwargs)\n",
    "        self.right._plot(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.left} + {self.right}\"\n",
    "\n",
    "\n",
    "class MultiplicativeTimeSeries(TimeSeriesModel):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def definition(self, *args, **kwargs):\n",
    "        return self.left.definition(*args, **kwargs) * (\n",
    "            1 + self.right.definition(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _predict(self, *args, **kwargs):\n",
    "        return self.left._predict(*args, **kwargs) * (\n",
    "            1 + self.right._predict(*args, **kwargs)\n",
    "        )\n",
    "\n",
    "    def _plot(self, *args, **kwargs):\n",
    "        self.left._plot(*args, **kwargs)\n",
    "        self.right._plot(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        left = f\"{self.left}\"\n",
    "        if type(self.left) is AdditiveTimeSeries:\n",
    "            left = f\"({self.left})\"\n",
    "\n",
    "        right = f\"{self.right}\"\n",
    "        if type(self.right) is AdditiveTimeSeries:\n",
    "            right = f\"({self.right})\"\n",
    "\n",
    "        return f\"{left} * {right}\"\n",
    "\n",
    "\n",
    "class LinearTrend(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_changepoints=25,\n",
    "        changepoint_range=0.8,\n",
    "        slope_mean=0,\n",
    "        slope_sd=5,\n",
    "        intercept_mean=0,\n",
    "        intercept_sd=5,\n",
    "        delta_mean=0,\n",
    "        delta_sd=0.05,\n",
    "        pool_cols=None,\n",
    "        pool_type=\"complete\",\n",
    "    ):\n",
    "        self.n_changepoints = n_changepoints\n",
    "        self.changepoint_range = changepoint_range\n",
    "        self.slope_mean = slope_mean\n",
    "        self.slope_sd = slope_sd\n",
    "        self.intercept_mean = intercept_mean\n",
    "        self.intercept_sd = intercept_sd\n",
    "        self.delta_mean = delta_mean\n",
    "        self.delta_sd = delta_sd\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"lt\"] = model_idxs.get(\"lt\", 0)\n",
    "        self.model_idx = model_idxs[\"lt\"]\n",
    "        model_idxs[\"lt\"] += 1\n",
    "\n",
    "        self.group, self.n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                sigma_slope = pm.HalfCauchy(\n",
    "                    f\"lt_{self.model_idx} - sigma_slope\", beta=self.slope_sd\n",
    "                )\n",
    "                offset_slope = pm.Normal(\n",
    "                    f\"lt_{self.model_idx} - offset_slope\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=self.n_groups,\n",
    "                )\n",
    "                slope = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - slope\", offset_slope * sigma_slope\n",
    "                )\n",
    "\n",
    "                delta_sd = self.delta_sd\n",
    "                if self.delta_sd is None:\n",
    "                    delta_sd = pm.Exponential(f\"lt_{self.model_idx} - tau\", 1.5)\n",
    "\n",
    "                sigma_delta = pm.HalfCauchy(\n",
    "                    f\"lt_{self.model_idx} - sigma_delta\", beta=delta_sd\n",
    "                )\n",
    "                offset_delta = pm.Laplace(\n",
    "                    f\"lt_{self.model_idx} - offset_delta\",\n",
    "                    0,\n",
    "                    1,\n",
    "                    shape=(self.n_groups, self.n_changepoints),\n",
    "                )\n",
    "                delta = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - delta\", offset_delta * sigma_delta\n",
    "                )\n",
    "            else:\n",
    "                slope = pm.Normal(\n",
    "                    f\"lt_{self.model_idx} - slope\",\n",
    "                    self.slope_mean,\n",
    "                    self.slope_sd,\n",
    "                    initval=initvals.get(\"slope\", None),\n",
    "                    shape=self.n_groups,\n",
    "                )\n",
    "\n",
    "                delta_sd = self.delta_sd\n",
    "                if self.delta_sd is None:\n",
    "                    delta_sd = pm.Exponential(f\"lt_{self.model_idx} - tau\", 1.5)\n",
    "\n",
    "                delta = pm.Laplace(\n",
    "                    f\"lt_{self.model_idx} - delta\",\n",
    "                    self.delta_mean,\n",
    "                    delta_sd,\n",
    "                    shape=(self.n_groups, self.n_changepoints),\n",
    "                )\n",
    "\n",
    "            intercept = pm.Normal(\n",
    "                f\"lt_{self.model_idx} - intercept\",\n",
    "                self.intercept_mean,\n",
    "                self.intercept_sd,\n",
    "                initval=initvals.get(\"intercept\", None),\n",
    "                shape=self.n_groups,\n",
    "            )\n",
    "\n",
    "            if self.pool_type == \"individual\":\n",
    "                ss = []\n",
    "                t = np.array(data[\"t\"])\n",
    "                for group_code in range(self.n_groups):\n",
    "                    series_data = data[self.group == group_code]\n",
    "                    hist_size = int(\n",
    "                        np.floor(series_data.shape[0] * self.changepoint_range)\n",
    "                    )\n",
    "                    cp_indexes = (\n",
    "                        np.linspace(0, hist_size - 1, self.n_changepoints + 1)\n",
    "                        .round()\n",
    "                        .astype(int)\n",
    "                    )\n",
    "                    ss.append(np.array(series_data.iloc[cp_indexes][\"t\"].tail(-1)))\n",
    "\n",
    "                self.s = np.stack(ss, axis=0)\n",
    "                A = (t[:, None] > self.s[self.group]) * 1\n",
    "\n",
    "                gamma = -self.s[self.group, :] * delta[self.group, :]\n",
    "                trend = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - trend\",\n",
    "                    (slope[self.group] + pm.math.sum(A * delta[self.group], axis=1)) * t\n",
    "                    + (intercept[self.group] + pm.math.sum(A * gamma, axis=1)),\n",
    "                )\n",
    "            else:\n",
    "                t = np.array(data[\"t\"])\n",
    "                hist_size = int(np.floor(data.shape[0] * self.changepoint_range))\n",
    "                cp_indexes = (\n",
    "                    np.linspace(0, hist_size - 1, self.n_changepoints + 1)\n",
    "                    .round()\n",
    "                    .astype(int)\n",
    "                )\n",
    "                self.s = np.array(data.iloc[cp_indexes][\"t\"].tail(-1))\n",
    "                A = (t[:, None] > self.s) * 1\n",
    "\n",
    "                gamma = -self.s * delta[self.group, :]\n",
    "                trend = pm.Deterministic(\n",
    "                    f\"lt_{self.model_idx} - trend\",\n",
    "                    (slope[self.group] + pm.math.sum(A * delta[self.group], axis=1)) * t\n",
    "                    + (intercept[self.group] + pm.math.sum(A * gamma, axis=1)),\n",
    "                )\n",
    "\n",
    "        return trend\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        if self.pool_type != \"individual\":\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s) * 1\n",
    "\n",
    "        for group_code in self.groups_.keys():\n",
    "            if self.pool_type == \"individual\":\n",
    "                s = self.s[group_code]\n",
    "                new_A = (np.array(future[\"t\"])[:, None] > self.s[group_code]) * 1\n",
    "            else:\n",
    "                s = self.s\n",
    "\n",
    "            forecasts.append(\n",
    "                np.array(\n",
    "                    (\n",
    "                        map_approx[f\"lt_{self.model_idx} - slope\"][group_code]\n",
    "                        + np.dot(\n",
    "                            new_A,\n",
    "                            map_approx[f\"lt_{self.model_idx} - delta\"][group_code],\n",
    "                        )\n",
    "                    )\n",
    "                    * future[\"t\"]\n",
    "                    + (\n",
    "                        map_approx[f\"lt_{self.model_idx} - intercept\"][group_code]\n",
    "                        + np.dot(\n",
    "                            new_A,\n",
    "                            (\n",
    "                                -s\n",
    "                                * map_approx[f\"lt_{self.model_idx} - delta\"][group_code]\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            future[f\"lt_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        if self.pool_type == \"individual\":\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s[self.group]) * 1\n",
    "        else:\n",
    "            new_A = (np.array(future[\"t\"])[:, None] > self.s) * 1\n",
    "\n",
    "        for group_code in self.groups_.keys():\n",
    "            delta = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - delta\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "            slope = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - slope\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "            intercept = (\n",
    "                trace[\"posterior\"][f\"lt_{self.model_idx} - intercept\"]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean(0)\n",
    "            )\n",
    "\n",
    "            forecasts.append(\n",
    "                (\n",
    "                    (slope + np.dot(new_A, delta.T)).T * future[\"t\"].to_numpy()\n",
    "                    + (intercept + np.dot(new_A, (-self.s * delta).T)).T\n",
    "                ).mean(0)\n",
    "            )\n",
    "            future[f\"lt_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"lt_{self.model_idx}\")\n",
    "        plt.grid()\n",
    "\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plt.plot(\n",
    "                future[\"ds\"],\n",
    "                future[f\"lt_{self.model_idx}_{group_code}\"],\n",
    "                lw=1,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LT(n={self.n_changepoints},r={self.changepoint_range},{self.pool_type})\"\n",
    "\n",
    "\n",
    "class FourierSeasonality(TimeSeriesModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        period,\n",
    "        series_order,\n",
    "        beta_mean=0,\n",
    "        beta_sd=10,\n",
    "        shrinkage_strength=100,\n",
    "        pool_cols=None,\n",
    "        pool_type=\"complete\",\n",
    "    ):\n",
    "        self.period = period\n",
    "        self.series_order = series_order\n",
    "        self.beta_mean = beta_mean\n",
    "        self.beta_sd = beta_sd\n",
    "        self.shrinkage_strength = shrinkage_strength\n",
    "\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def _fourier_series(self, data):\n",
    "        # convert to days since epoch\n",
    "        NANOSECONDS_TO_SECONDS = 1000 * 1000 * 1000\n",
    "        t = (\n",
    "            data[\"ds\"].to_numpy(dtype=np.int64)\n",
    "            // NANOSECONDS_TO_SECONDS\n",
    "            / (3600 * 24.0)\n",
    "        )\n",
    "\n",
    "        x_T = t * np.pi * 2\n",
    "        fourier_components = np.empty((data[\"ds\"].shape[0], 2 * self.series_order))\n",
    "        for i in range(self.series_order):\n",
    "            c = x_T * (i + 1) / self.period\n",
    "            fourier_components[:, 2 * i] = np.sin(c)\n",
    "            fourier_components[:, (2 * i) + 1] = np.cos(c)\n",
    "\n",
    "        return fourier_components\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"fs\"] = model_idxs.get(\"fs\", 0)\n",
    "        self.model_idx = model_idxs[\"fs\"]\n",
    "        model_idxs[\"fs\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        x = self._fourier_series(data)\n",
    "        beta_initval = initvals.get(\"beta\", None)\n",
    "        if beta_initval is not None:\n",
    "            beta_initval = np.array([beta_initval] * 2 * self.series_order)\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                # shift_t = pm.Uniform(\n",
    "                #     f\"fs_{self.model_idx} - shift_t(p={self.period},n={self.series_order})\",\n",
    "                #     lower=0,\n",
    "                #     upper=self.period,\n",
    "                #     shape=n_groups,\n",
    "                # )\n",
    "                mu_beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - beta_mu(p={self.period},n={self.series_order})\",\n",
    "                    mu=self.beta_mean,\n",
    "                    sigma=self.beta_sd,\n",
    "                    shape=2 * self.series_order,\n",
    "                    initval=beta_initval,\n",
    "                )\n",
    "                sigma_beta = pm.HalfNormal(\n",
    "                    f\"fs_{self.model_idx} - beta_sigma(p={self.period},n={self.series_order})\",\n",
    "                    sigma=self.beta_sd / self.shrinkage_strength,\n",
    "                    shape=2 * self.series_order,\n",
    "                )\n",
    "                offset_beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - offset_beta(p={self.period},n={self.series_order})\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=(n_groups, 2 * self.series_order),\n",
    "                )\n",
    "\n",
    "                beta = pm.Deterministic(\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                    mu_beta + offset_beta * sigma_beta,\n",
    "                )\n",
    "            else:\n",
    "                beta = pm.Normal(\n",
    "                    f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\",\n",
    "                    mu=self.beta_mean,\n",
    "                    sigma=self.beta_sd,\n",
    "                    shape=(n_groups, 2 * self.series_order),\n",
    "                    initval=beta_initval,\n",
    "                )\n",
    "\n",
    "        return pm.math.sum(x * beta[group], axis=1)\n",
    "\n",
    "    def _det_seasonality_posterior(self, beta, x):\n",
    "        return np.dot(x, beta.T)\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                self._det_seasonality_posterior(\n",
    "                    map_approx[\n",
    "                        f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                    ][group_code],\n",
    "                    self._fourier_series(future),\n",
    "                )\n",
    "            )\n",
    "            future[f\"fs_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                self._det_seasonality_posterior(\n",
    "                    trace[\"posterior\"][\n",
    "                        f\"fs_{self.model_idx} - beta(p={self.period},n={self.series_order})\"\n",
    "                    ]\n",
    "                    .to_numpy()[:, :, group_code]\n",
    "                    .mean(0),\n",
    "                    self._fourier_series(future),\n",
    "                ).T.mean(0)\n",
    "            )\n",
    "            future[f\"fs_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        date = future[\"ds\"] if self.period > 7 else future[\"ds\"].dt.day_name()\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"fs_{self.model_idx} - p={self.period},n={self.series_order}\")\n",
    "        plt.grid()\n",
    "\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plt.plot(\n",
    "                date[-int(self.period) :],\n",
    "                future[f\"fs_{self.model_idx}_{group_code}\"][-int(self.period) :],\n",
    "                lw=1,\n",
    "                label=group_name,\n",
    "            )\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"FS(p={self.period},n={self.series_order},{self.pool_type})\"\n",
    "\n",
    "\n",
    "class Constant(TimeSeriesModel):\n",
    "    def __init__(self, lower, upper, pool_cols=None, pool_type=\"complete\"):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.pool_cols = pool_cols\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def definition(self, model, data, initvals, model_idxs):\n",
    "        model_idxs[\"c\"] = model_idxs.get(\"c\", 0)\n",
    "        self.model_idx = model_idxs[\"c\"]\n",
    "        model_idxs[\"c\"] += 1\n",
    "\n",
    "        group, n_groups, self.groups_ = get_group_definition(\n",
    "            data, self.pool_cols, self.pool_type\n",
    "        )\n",
    "\n",
    "        with model:\n",
    "            if self.pool_type == \"partial\":\n",
    "                mu_c = pm.Uniform(\n",
    "                    f\"c_{self.model_idx} - mu_c(l={self.lower},u={self.upper})\",\n",
    "                    lower=self.lower,\n",
    "                    upper=self.upper,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "                offset_c = pm.Normal(\n",
    "                    f\"c_{self.model_idx} - offset_c(l={self.lower},u={self.upper})\",\n",
    "                    mu=0,\n",
    "                    sigma=1,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "                c = pm.Deterministic(\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\",\n",
    "                    mu_c + offset_c,\n",
    "                )\n",
    "            else:\n",
    "                c = pm.Uniform(\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\",\n",
    "                    lower=self.lower,\n",
    "                    upper=self.upper,\n",
    "                    shape=n_groups,\n",
    "                )\n",
    "\n",
    "        return c[group]\n",
    "\n",
    "    def _predict_map(self, future, map_approx):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                np.ones_like(future[\"t\"])\n",
    "                * map_approx[f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\"][\n",
    "                    group_code\n",
    "                ]\n",
    "            )\n",
    "            future[f\"c_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _predict_mcmc(self, future, trace):\n",
    "        forecasts = []\n",
    "        for group_code in self.groups_.keys():\n",
    "            forecasts.append(\n",
    "                np.ones_like(future[\"t\"])\n",
    "                * trace[\"posterior\"][\n",
    "                    f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\"\n",
    "                ]\n",
    "                .to_numpy()[:, :, group_code]\n",
    "                .mean()\n",
    "            )\n",
    "            future[f\"c_{self.model_idx}_{group_code}\"] = forecasts[-1]\n",
    "\n",
    "        return np.vstack(forecasts)\n",
    "\n",
    "    def _plot(self, plot_params, future, data, y_max, y_true=None):\n",
    "        plot_params[\"idx\"] += 1\n",
    "        plt.subplot(100, 1, plot_params[\"idx\"])\n",
    "        plt.title(f\"c_{self.model_idx} - c(l={self.lower},u={self.upper})\")\n",
    "\n",
    "        plot_data = []\n",
    "        for group_code, group_name in self.groups_.items():\n",
    "            plot_data.append(\n",
    "                (group_name, future[f\"c_{self.model_idx}_{group_code}\"][0])\n",
    "            )\n",
    "\n",
    "        plt.bar(*zip(*plot_data))\n",
    "        plt.axhline(0, c=\"k\", linewidth=3)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"C(l={self.lower},u={self.upper},{self.pool_type})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [\"^W5000\", \"^GSPC\", \"^IXIC\", \"^DJI\"]\n",
    "\n",
    "gspc_tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"AMZN\", \"FB\", \"TSLA\", \"GOOGL\", \"GOOG\", \"JNJ\", \"JPM\", \"V\",\n",
    "    \"PG\", \"UNH\", \"DIS\", \"NVDA\", \"MA\", \"HD\", \"PYPL\", \"VZ\", \"ADBE\", \"CMCSA\",\n",
    "    \"NFLX\", \"BAC\", \"KO\", \"MRK\", \"PEP\", \"T\", \"PFE\", \"INTC\", \"CRM\", \"WMT\", \"ABT\",\n",
    "    \"ABBV\", \"CSCO\", \"TMO\", \"NKE\", \"AVGO\", \"XOM\", \"QCOM\", \"COST\", \"ACN\", \"CVX\",\n",
    "    \"MCD\", \"MDT\", \"NEE\", \"TXN\", \"HON\", \"DHR\", \"UNP\", \"BMY\", \"LIN\", \"LLY\",\n",
    "    \"AMGN\", \"PM\", \"C\", \"SBUX\", \"WFC\", \"ORCL\", \"UPS\", \"LOW\", \"BA\", \"IBM\", \"AMD\",\n",
    "    \"RTX\", \"NOW\", \"BLK\", \"MMM\", \"INTU\", \"AMT\", \"CAT\", \"MS\", \"CHTR\", \"ISRG\",\n",
    "    \"GE\", \"BKNG\", \"GS\", \"CVS\", \"TGT\", \"FIS\", \"LMT\", \"DE\", \"MU\", \"MDLZ\", \"TJX\",\n",
    "    \"SYK\", \"ANTM\", \"SCHW\", \"SPGI\", \"AXP\", \"AMAT\", \"TMUS\", \"ZTS\", \"MO\", \"ADP\",\n",
    "    \"CI\", \"PLD\", \"CL\", \"GILD\", \"BDX\", \"ATVI\", \"CB\", \"CSX\", \"CCI\", \"LRCX\",\n",
    "    \"DUK\", \"ADSK\", \"FISV\", \"CME\", \"SO\", \"ICE\", \"TFC\", \"GPN\", \"USB\", \"EQIX\",\n",
    "    \"PNC\", \"FDX\", \"VRTX\", \"D\", \"APD\", \"NSC\", \"EL\", \"SHW\", \"MMC\", \"ITW\", \"PGR\",\n",
    "    \"EW\", \"ADI\", \"HUM\", \"ILMN\", \"ECL\", \"GM\", \"DD\", \"DG\", \"BSX\", \"REGN\", \"AON\",\n",
    "    \"NEM\", \"EMR\", \"ETN\", \"NOC\", \"MCO\", \"KMB\", \"WM\", \"COF\", \"ROP\", \"CTSH\",\n",
    "    \"ROST\", \"HCA\", \"TWTR\", \"COP\", \"IDXX\", \"EA\", \"AEP\", \"EXC\", \"DOW\", \"BAX\",\n",
    "    \"TEL\", \"KLAC\", \"LHX\", \"SNPS\", \"APH\", \"DLR\", \"CMG\", \"ALGN\", \"CDNS\", \"SYY\",\n",
    "    \"FCX\", \"BIIB\", \"STZ\", \"MSCI\", \"SRE\", \"A\", \"MCHP\", \"GIS\", \"MET\", \"TRV\",\n",
    "    \"DXCM\", \"APTV\", \"PSA\", \"PH\", \"MAR\", \"XEL\", \"TT\", \"CNC\", \"XLNX\", \"GD\", \"BK\",\n",
    "    \"F\", \"IQV\", \"TROW\", \"ALXN\", \"MNST\", \"PPG\", \"HPQ\", \"VRSK\", \"JCI\", \"TDG\",\n",
    "    \"CMI\", \"INFO\", \"ALL\", \"EBAY\", \"ORLY\", \"YUM\", \"AIG\", \"ZBH\", \"SBAC\", \"ANSS\",\n",
    "    \"CTAS\", \"PRU\", \"HLT\", \"RMD\", \"CARR\", \"PSX\", \"BLL\", \"SLB\", \"PCAR\", \"PAYX\",\n",
    "    \"ES\", \"PEG\", \"ROK\", \"EOG\", \"AFL\", \"WEC\", \"CTVA\", \"MSI\", \"WBA\", \"SWK\",\n",
    "    \"ADM\", \"FAST\", \"SPG\", \"MCK\", \"AME\", \"AWK\", \"DFS\", \"LUV\", \"OTIS\", \"GLW\",\n",
    "    \"AZO\", \"VFC\", \"WLTW\", \"MTD\", \"WELL\", \"MPC\", \"KMI\", \"CPRT\", \"STT\", \"DAL\",\n",
    "    \"FRC\", \"CLX\", \"DLTR\", \"SWKS\", \"WY\", \"ED\", \"KR\", \"KEYS\", \"WMB\", \"CERN\",\n",
    "    \"TTWO\", \"FTV\", \"AJG\", \"EIX\", \"MKC\", \"MXIM\", \"LYB\", \"DTE\", \"EFX\", \"VLO\",\n",
    "    \"BBY\", \"AMP\", \"DHI\", \"FLT\", \"VTRS\", \"HSY\", \"KHC\", \"AVB\", \"PAYC\", \"ETSY\",\n",
    "    \"O\", \"VRSN\", \"PPL\", \"CHD\", \"MKTX\", \"ARE\", \"VIAC\", \"CBRE\", \"LEN\", \"WST\",\n",
    "    \"ZBRA\", \"EQR\", \"RSG\", \"SIVB\", \"FTNT\", \"ETR\", \"TER\", \"LH\", \"VMC\", \"FITB\",\n",
    "    \"LVS\", \"IP\", \"NTRS\", \"AEE\", \"TFX\", \"KSU\", \"QRVO\", \"TSN\", \"SYF\", \"CDW\",\n",
    "    \"ODFL\", \"PXD\", \"HOLX\", \"AMCR\", \"GWW\", \"VTR\", \"XYL\", \"DOV\", \"EXPE\", \"GRMN\",\n",
    "    \"COO\", \"CAG\", \"BR\", \"MLM\", \"TYL\", \"HIG\", \"CMS\", \"CTLT\", \"AKAM\", \"OKE\",\n",
    "    \"IR\", \"WDC\", \"URI\", \"HAL\", \"FE\", \"TSCO\", \"MTB\", \"PEAK\", \"INCY\", \"ULTA\",\n",
    "    \"STE\", \"CCL\", \"EXPD\", \"PKI\", \"NUE\", \"DGX\", \"KEY\", \"CTXS\", \"VAR\", \"K\",\n",
    "    \"ANET\", \"CAH\", \"ALB\", \"AES\", \"DRI\", \"KMX\", \"RF\", \"ESS\", \"WAT\", \"CFG\",\n",
    "    \"HPE\", \"NDAQ\", \"CE\", \"DPZ\", \"IEX\", \"EXR\", \"POOL\", \"FMC\", \"DRE\", \"NTAP\",\n",
    "    \"ABMD\", \"OXY\", \"MAA\", \"GPC\", \"TDY\", \"HES\", \"ABC\", \"MAS\", \"IT\", \"NVR\",\n",
    "    \"TIF\", \"J\", \"LDOS\", \"BKR\", \"STX\", \"RCL\", \"EMN\", \"OMC\", \"BXP\", \"SJM\", \"WAB\",\n",
    "    \"HRL\", \"PKG\", \"CINF\", \"AVY\", \"MGM\", \"LNT\", \"HBAN\", \"CHRW\", \"PFG\", \"UAL\",\n",
    "    \"EVRG\", \"BIO\", \"JKHY\", \"NLOK\", \"HAS\", \"ATO\", \"FBHS\", \"CNP\", \"RJF\", \"IFF\",\n",
    "    \"PHM\", \"LW\", \"CXO\", \"XRAY\", \"WRK\", \"JBHT\", \"UDR\", \"WHR\", \"HWM\", \"TXT\",\n",
    "    \"WYNN\", \"FFIV\", \"ALLE\", \"AAP\", \"UHS\", \"L\", \"LYV\", \"HST\", \"CBOE\", \"PWR\",\n",
    "    \"LKQ\", \"FOXA\", \"CPB\", \"AAL\", \"LUMN\", \"HSIC\", \"BWA\", \"RE\", \"WRB\", \"SNA\",\n",
    "    \"IPG\", \"NRG\", \"GL\", \"LNC\", \"WU\", \"PNW\", \"PNR\", \"NI\", \"LB\", \"DVA\", \"ROL\",\n",
    "    \"TPR\", \"TAP\", \"IRM\", \"MHK\", \"CF\", \"AIZ\", \"NCLH\", \"NWL\", \"DISH\", \"IPGP\",\n",
    "    \"MOS\", \"CMA\", \"DISCK\", \"FANG\", \"NLSN\", \"AOS\", \"JNPR\", \"REG\", \"ZION\", \"RHI\",\n",
    "    \"SEE\", \"NWSA\", \"HII\", \"BEN\", \"PVH\", \"IVZ\", \"DXC\", \"COG\", \"KIM\", \"ALK\",\n",
    "    \"PRGO\", \"DVN\", \"LEG\", \"FRT\", \"VNO\", \"FLIR\", \"PBCT\", \"APA\", \"NOV\", \"MRO\",\n",
    "    \"HBI\", \"RL\", \"DISCA\", \"FLS\", \"UNM\", \"VNT\", \"FOX\", \"SLG\", \"GPS\", \"FTI\",\n",
    "    \"XRX\", \"HFC\", \"UAA\", \"UA\", \"NWS\"\n",
    "]\n",
    "\n",
    "dji_tickers = [\n",
    "    \"DIS\", \"WMT\", \"DOW\", \"NKE\", \"CRM\", \"HD\", \"V\", \"MSFT\", \"MMM\", \"CSCO\", \"KO\",\n",
    "    \"AAPL\", \"HON\", \"JNJ\", \"TRV\", \"PG\", \"CVX\", \"VZ\", \"CAT\", \"BA\", \"AMGN\", \"IBM\",\n",
    "    \"AXP\", \"JPM\", \"WBA\", \"MCD\", \"MRK\", \"GS\", \"UNH\", \"INTC\"\n",
    "]\n",
    "\n",
    "ixic_tickers = [\n",
    "    \"FEYE\", \"ATEC\", \"SLAB\", \"CMRX\", \"NVCR\", \"FNLC\", \"NMRK\", \"SCOR\", \"AGLE\",\n",
    "    \"FARO\", \"OLMA\", \"TSLA\", \"FRTA\", \"AKTX\", \"KLXE\", \"CVCO\", \"NVCN\", \"EXAS\",\n",
    "    \"SDC\", \"BBQ\", \"IFRX\", \"CIIC\", \"BBI\", \"FNKO\", \"TWST\", \"FARM\", \"ACCD\",\n",
    "    \"NMRD\", \"FRSX\", \"OPTT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(tickers, start=\"1980-01-01\", end=\"2020-01-01\"):\n",
    "    data = yfinance.download(\n",
    "        tickers,\n",
    "        interval=\"1d\",\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "    downloaded_tickers = {col[1] for col in data.columns}\n",
    "    dfs = []\n",
    "    for ticker in downloaded_tickers:\n",
    "        df = pd.DataFrame(\n",
    "            data={\n",
    "                \"open\": data[\"Open\"][ticker].to_numpy(),\n",
    "                \"high\": data[\"High\"][ticker].to_numpy(),\n",
    "                \"low\": data[\"Low\"][ticker].to_numpy(),\n",
    "                \"close\": data[\"Close\"][ticker].to_numpy(),\n",
    "                \"typical_price\": (\n",
    "                    (\n",
    "                        data[\"Open\"][ticker]\n",
    "                        + data[\"High\"][ticker]\n",
    "                        + data[\"Low\"][ticker]\n",
    "                        + data[\"Close\"][ticker]\n",
    "                    )\n",
    "                    / 4\n",
    "                ).to_numpy(),\n",
    "                \"volume\": data[\"Volume\"][ticker].to_numpy(),\n",
    "            },\n",
    "            index=data[\"Close\"][ticker].index,\n",
    "        )\n",
    "\n",
    "        full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"D\")\n",
    "        df = df.reindex(full_date_range).interpolate()\n",
    "        df[\"ds\"] = df.index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df[\"series\"] = ticker\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_df(\n",
    "    start,\n",
    "    window,\n",
    "    horizon,\n",
    "    dfs,\n",
    "    for_prophet=False,\n",
    "    y_col=\"typical_price\",\n",
    "    perform_scaling=True,\n",
    "):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    scales = []\n",
    "    for df in dfs:\n",
    "        train_df = df[start : start + window].copy()\n",
    "        test_df = df[start + window : start + window + horizon].copy()\n",
    "        if train_df.isna().any().any() or test_df.isna().any().any():\n",
    "            continue\n",
    "\n",
    "        train_df[\"y\"] = train_df[y_col]\n",
    "        test_df[\"y\"] = test_df[y_col]\n",
    "\n",
    "        if perform_scaling:\n",
    "            scales.append(train_df[y_col].max())\n",
    "            train_df[\"y\"] = train_df[y_col] / scales[-1]\n",
    "            test_df[\"y\"] = test_df[y_col] / scales[-1]\n",
    "\n",
    "        train_dfs.append(train_df)\n",
    "        test_dfs.append(test_df)\n",
    "\n",
    "    if len(train_dfs) == 0:\n",
    "        return None\n",
    "\n",
    "    if for_prophet:\n",
    "        return train_dfs, test_dfs, scales\n",
    "\n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs), scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_df_around_point(\n",
    "    window,\n",
    "    horizon,\n",
    "    dfs,\n",
    "    point=\"2009-09-01\",\n",
    "    for_prophet=False,\n",
    "    y_col=\"typical_price\",\n",
    "    perform_scaling=True,\n",
    "):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    scales = []\n",
    "\n",
    "    for df in dfs:\n",
    "        point_idx = len(dfs[0][dfs[0][\"ds\"] < point])\n",
    "        check = generate_train_test_df(\n",
    "            start=point_idx - window,\n",
    "            window=window,\n",
    "            horizon=horizon,\n",
    "            dfs=[df],\n",
    "            for_prophet=for_prophet,\n",
    "            y_col=y_col,\n",
    "            perform_scaling=perform_scaling,\n",
    "        )\n",
    "        if check is None:\n",
    "            continue\n",
    "\n",
    "        train_df, test_df, scale = check\n",
    "\n",
    "        scales += scale\n",
    "\n",
    "        if for_prophet:\n",
    "            train_dfs += train_df\n",
    "            test_dfs += test_df\n",
    "        else:\n",
    "            train_dfs.append(train_df)\n",
    "            test_dfs.append(test_df)\n",
    "\n",
    "    if len(train_dfs) == 0:\n",
    "        return None\n",
    "    \n",
    "    if for_prophet:\n",
    "        return train_dfs, test_dfs, scales\n",
    "    \n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs), scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_components = [\n",
    "    [LinearTrend(pool_cols=\"series\", pool_type=pt) for pt in [\"individual\", \"partial\"]],\n",
    "    [\n",
    "        FourierSeasonality(\n",
    "            period=365.25, series_order=10, pool_cols=\"series\", pool_type=pt\n",
    "        )\n",
    "        for pt in [\"individual\", \"partial\"]\n",
    "    ],\n",
    "    [\n",
    "        FourierSeasonality(\n",
    "            period=91.3125, series_order=n, pool_cols=\"series\", pool_type=pt\n",
    "        )\n",
    "        for n in range(7, 10)\n",
    "        for pt in [\"individual\", \"partial\"]\n",
    "    ],\n",
    "    [\n",
    "        FourierSeasonality(\n",
    "            period=30.4375, series_order=n, pool_cols=\"series\", pool_type=pt\n",
    "        )\n",
    "        for n in range(4, 7)\n",
    "        for pt in [\"individual\", \"partial\"]\n",
    "    ],\n",
    "    [\n",
    "        FourierSeasonality(\n",
    "            period=7, series_order=3, pool_cols=\"series\", pool_type=pt\n",
    "        )\n",
    "        for pt in [\"individual\", \"partial\"]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [(0, [mc]) for mc in model_components[0]]\n",
    "models = []\n",
    "\n",
    "while len(q):\n",
    "    level, model = q.pop(0)\n",
    "    if level + 1 == len(model_components):\n",
    "        models.append(model)\n",
    "        continue\n",
    "\n",
    "    mcs = model_components[level + 1]\n",
    "    for mc in mcs:\n",
    "        # if mc.pool_type == \"partial\":\n",
    "        #     q.append(\n",
    "        #         (\n",
    "        #             level + 1,\n",
    "        #             model\n",
    "        #             + [\n",
    "        #                 Constant(\n",
    "        #                     lower=-1, upper=1, pool_cols=\"series\", pool_type=\"partial\"\n",
    "        #                 )\n",
    "        #                 * mc\n",
    "        #             ],\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        q.append((level + 1, model + [mc]))\n",
    "        q.append((level + 1, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_models(models):\n",
    "    s = None\n",
    "    for model in models:\n",
    "        if s is None:\n",
    "            s = model\n",
    "        else:\n",
    "            s += model\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model[0] * sum_models(model[1:]) if len(model) > 1 else model[0] for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_models = {\"\"}\n",
    "final_models = []\n",
    "for model in models:\n",
    "    str_model = str(model)\n",
    "    if str_model in str_models:\n",
    "        continue\n",
    "\n",
    "    str_models.add(str_model)\n",
    "    final_models.append(model)\n",
    "\n",
    "len(final_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n"
     ]
    }
   ],
   "source": [
    "dfs = fetch_data([\"^GSPC\", \"MSFT\", \"META\", \"AAPL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>typical_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>ds</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-01-06</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14604</th>\n",
       "      <td>70.558937</td>\n",
       "      <td>71.249695</td>\n",
       "      <td>69.831825</td>\n",
       "      <td>70.239006</td>\n",
       "      <td>70.469866</td>\n",
       "      <td>146266000.0</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14605</th>\n",
       "      <td>70.424825</td>\n",
       "      <td>71.146283</td>\n",
       "      <td>69.597534</td>\n",
       "      <td>70.377965</td>\n",
       "      <td>70.386652</td>\n",
       "      <td>145548800.0</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14606</th>\n",
       "      <td>70.290713</td>\n",
       "      <td>71.042872</td>\n",
       "      <td>69.363243</td>\n",
       "      <td>70.516925</td>\n",
       "      <td>70.303438</td>\n",
       "      <td>144831600.0</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14607</th>\n",
       "      <td>70.156601</td>\n",
       "      <td>70.939461</td>\n",
       "      <td>69.128952</td>\n",
       "      <td>70.655884</td>\n",
       "      <td>70.220224</td>\n",
       "      <td>144114400.0</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14608</th>\n",
       "      <td>70.270515</td>\n",
       "      <td>71.179405</td>\n",
       "      <td>70.171143</td>\n",
       "      <td>71.172134</td>\n",
       "      <td>70.698299</td>\n",
       "      <td>100805600.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14609 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            open       high        low      close  typical_price       volume  \\\n",
       "0            NaN        NaN        NaN        NaN            NaN          NaN   \n",
       "1            NaN        NaN        NaN        NaN            NaN          NaN   \n",
       "2            NaN        NaN        NaN        NaN            NaN          NaN   \n",
       "3            NaN        NaN        NaN        NaN            NaN          NaN   \n",
       "4            NaN        NaN        NaN        NaN            NaN          NaN   \n",
       "...          ...        ...        ...        ...            ...          ...   \n",
       "14604  70.558937  71.249695  69.831825  70.239006      70.469866  146266000.0   \n",
       "14605  70.424825  71.146283  69.597534  70.377965      70.386652  145548800.0   \n",
       "14606  70.290713  71.042872  69.363243  70.516925      70.303438  144831600.0   \n",
       "14607  70.156601  70.939461  69.128952  70.655884      70.220224  144114400.0   \n",
       "14608  70.270515  71.179405  70.171143  71.172134      70.698299  100805600.0   \n",
       "\n",
       "              ds series  \n",
       "0     1980-01-02   AAPL  \n",
       "1     1980-01-03   AAPL  \n",
       "2     1980-01-04   AAPL  \n",
       "3     1980-01-05   AAPL  \n",
       "4     1980-01-06   AAPL  \n",
       "...          ...    ...  \n",
       "14604 2019-12-27   AAPL  \n",
       "14605 2019-12-28   AAPL  \n",
       "14606 2019-12-29   AAPL  \n",
       "14607 2019-12-30   AAPL  \n",
       "14608 2019-12-31   AAPL  \n",
       "\n",
       "[14609 rows x 8 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
